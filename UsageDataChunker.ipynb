{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRITICAL STEPS BEFORE PROCESSING ONBOARDING CUSTOMER USAGE DATA WITH THIS SCRIPT: \n",
    "### Identify the below columns within each data artifact\n",
    "### Update header naming conventions to match the below spelling and capitalization EXACTLY\n",
    "\n",
    "#### USAGE DATA FILE INPUTS NEEDED:\n",
    "    BUSN Unit\n",
    "    HELPERKEY\n",
    "    ORDER DATE\n",
    "    ORDER UOM\n",
    "    ORDER QTY\n",
    "    Cust UOM (optional)\n",
    "    Cust UOM Conv Factor (optional)\n",
    "\n",
    "#### PXR SCORECARD INPUTS NEEDED:\n",
    "    ADD/CHANGE DATE (ATS)\n",
    "    READY FOR RELEASE TO ATS/PURCHASING\n",
    "    SCORECARD CATEGORY\n",
    "    GO FORWARD SKU TO USE\n",
    "\n",
    "#### DC/ACCOUNT METADATA FILE INPUTS NEEDED:\n",
    "    BUSN Unit\n",
    "    LOC\n",
    "    OM ACCOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import math\n",
    "import oracledb\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "#### Enter necessary metadata here ####\n",
    "project_folder = '2024-12-09 University Hospital ABC'\n",
    "scorecard_date_prev = '2024-10-24'          # Enter date of the last scorecard used in ATS processing\n",
    "scorecard_date = '2024-11-06'               # Enter date of scorecard information needed to be processed now\n",
    "usageformat = 'PO'                          # Either 'PO' or 'monthly'\n",
    "numberofmonths = 12                          # enter the number of months, if data is in a monthly format\n",
    "ATS_workflow_type = 'update'                  # Either 'initial' or 'update'\n",
    "ATS_workflow_type_previousfile = 'update'     # Either 'initial' or 'update'\n",
    "\n",
    "# Capture the customer name using regex\n",
    "customer_name = re.search(r'\\d{4}-\\d{2}-\\d{2}\\s(.+)', project_folder).group(1)\n",
    "\n",
    "# Base directory path\n",
    "base_dir = 'C:/your/filepath/here/'\n",
    "projectinputs_folder = os.path.join(base_dir, project_folder, 'Inputs')\n",
    "projectoutputs_folder = os.path.join(base_dir, project_folder, 'Outputs')\n",
    "\n",
    "# Construct the file paths\n",
    "if ATS_workflow_type == 'initial':\n",
    "    data_usage_filepath = os.path.join(projectinputs_folder, f'{customer_name}_Usage_Initial.xlsx')\n",
    "elif ATS_workflow_type == 'update':\n",
    "    data_usage_filepath = os.path.join(projectoutputs_folder, f'{customer_name}_ATSFile_{ATS_workflow_type_previousfile}_{scorecard_date_prev}.xlsx')\n",
    "\n",
    "data_scorecard_filepath = os.path.join(projectinputs_folder, f'{customer_name}_Scorecard_{scorecard_date}.xlsb')\n",
    "df_metadata_filepath = os.path.join(projectinputs_folder, f'{customer_name}_DC_Acct_Data.xlsx')\n",
    "\n",
    "# Function to read Excel files with error handling\n",
    "def read_excel_file(filepath, **kwargs):\n",
    "    try:\n",
    "        return pd.read_excel(filepath, **kwargs)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found - {filepath}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Read in necessary data artifacts\n",
    "data_usage = read_excel_file(data_usage_filepath, sheet_name='Raw Usage', dtype={'MMIS ID': str})\n",
    "data_scorecard = read_excel_file(data_scorecard_filepath, sheet_name='Raw Data', engine='pyxlsb')\n",
    "df_metadata = read_excel_file(df_metadata_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clean scorecard data\n",
    "\n",
    "# Make a copy dataframe of raw PXR scorecard data\n",
    "df_scorecard = data_scorecard.copy()\n",
    "\n",
    "# Rename the primary SCORECARD CATEGORY column\n",
    "df_scorecard.rename(\n",
    "    columns={col: \"SCORECARD CATEGORY\" for col in df_scorecard.filter(like=\"SCORECARD CATEGORY\").columns if \"PREVIOUS\" not in col},\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# Define new column names with the scorecard date prefix\n",
    "new_column_names = {\n",
    "    'ADD/CHANGE DATE (ATS)': f'{scorecard_date} ADD/CHANGE DATE (ATS)',\n",
    "    'READY FOR RELEASE TO ATS/PURCHASING': f'{scorecard_date} READY RELEASE ATS',\n",
    "    'SCORECARD CATEGORY': f'{scorecard_date} SCORECARD CATEGORY',\n",
    "    'GO FORWARD SKU TO USE': f'{scorecard_date} OM SKU',   \n",
    "}\n",
    "\n",
    "# Apply new column names\n",
    "df_scorecard.rename(columns=new_column_names, inplace=True)\n",
    "\n",
    "# Filter for only necessary scorecard columns\n",
    "df_scorecard = df_scorecard[['HELPERKEY', f'{scorecard_date} READY RELEASE ATS',\n",
    "                             f'{scorecard_date} ADD/CHANGE DATE (ATS)',\n",
    "                             f'{scorecard_date} SCORECARD CATEGORY',\n",
    "                             f'{scorecard_date} OM SKU']]\n",
    "\n",
    "# Clean and change datatypes of given columns. Make everything upper case, trim leading and trailing spaces around OM SKUs\n",
    "df_scorecard['HELPERKEY'] = df_scorecard['HELPERKEY'].astype('str').str.upper()\n",
    "df_scorecard[f'{scorecard_date} READY RELEASE ATS'] = df_scorecard[f'{scorecard_date} READY RELEASE ATS'].str.upper()\n",
    "df_scorecard[f'{scorecard_date} OM SKU'] = df_scorecard[f'{scorecard_date} OM SKU'].str.strip()\n",
    "\n",
    "# Sort dataframe to put all 'YES' at the top\n",
    "df_scorecard.sort_values(by=f'{scorecard_date} READY RELEASE ATS', ascending=False, inplace=True)\n",
    "\n",
    "# Drop duplicate HELPERKEYs from scorecard. If this step is omitted, usage data will be duplicated due to data joins later on in this script\n",
    "df_scorecard.drop_duplicates(subset='HELPERKEY', keep='first', inplace=True)\n",
    "\n",
    "# Create dataframe with unique OM SKUs that are 'YES' for ready release\n",
    "df_unique_ready_release_om_skus = (\n",
    "    df_scorecard[df_scorecard[f'{scorecard_date} READY RELEASE ATS'] == 'YES']\n",
    "    .drop_duplicates(subset=[f'{scorecard_date} OM SKU'])\n",
    "    [[f'{scorecard_date} OM SKU']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OM SKUs not found in PRODUCT_DIM table:  Empty DataFrame\n",
      "Columns: [2024-11-06 OM SKU]\n",
      "Index: []\n",
      "# of SKUs from data pull:  1259\n"
     ]
    }
   ],
   "source": [
    "# Function to chunk a dataframe into chunks of specified size\n",
    "def chunk_list(dataframe, chunk_size=1000):\n",
    "    return [dataframe[i:i + chunk_size] for i in range(0, len(dataframe), chunk_size)]\n",
    "\n",
    "# Function to replace specific values in a column\n",
    "def replace_values(col):\n",
    "    return col.replace(['u', 0], np.nan)\n",
    "\n",
    "# Establish the database connection using a context manager to ensure it closes properly\n",
    "dsn_tns = oracledb.makedsn('hostnamehere', '1234', service_name='DATABASE_NAME_HERE')\n",
    "\n",
    "with oracledb.connect(user='usernamehere', password='pswdhere', dsn=dsn_tns) as conn:\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Create empty lists to hold the SQL query results for later concatenation\n",
    "    results_by = []\n",
    "    results_edw = []\n",
    "\n",
    "    # Break up unique_skus into chunks of 1000\n",
    "    chunks = chunk_list(df_unique_ready_release_om_skus, chunk_size=1000)\n",
    "\n",
    "    # Create a list of unique OM DCs\n",
    "    unique_dcs = ', '.join([f\"'{dc}'\" for dc in df_metadata['LOC'].unique()])\n",
    "\n",
    "    ### Run SQL query for BY data in chunks of 1000 OM SKUs\n",
    "    for chunk in chunks:\n",
    "        unique_skus_str = ', '.join([f\"'{sku}'\" for sku in chunk[f'{scorecard_date} OM SKU']])\n",
    "\n",
    "        query_by = f\"\"\"\n",
    "        SELECT\n",
    "            s.loc,\n",
    "            s.item AS \"{scorecard_date} OM SKU\",\n",
    "            s.UDC_4WEEKFCST AS \"DC 4wk fcst\",\n",
    "            s.planleadtime/1440 AS \"Days Lead Time\",\n",
    "            s.iflag\n",
    "        FROM SCPDBINSTANCE.SKU s\n",
    "        WHERE s.loc IN ({unique_dcs})\n",
    "        AND s.item IN ({unique_skus_str})\n",
    "        ORDER BY s.item\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            cur.execute(query_by)\n",
    "            rows = cur.fetchall()\n",
    "            column_names = [column[0] for column in cur.description]\n",
    "            results_by.append(pd.DataFrame(rows, columns=column_names))\n",
    "        except Exception as e:\n",
    "            print(f\"Error during BY query execution: {e}\")\n",
    "\n",
    "    # Concatenate all chunks into one DataFrame\n",
    "    df_by = pd.concat(results_by, ignore_index=True)\n",
    "\n",
    "    ### Run SQL query for EDW data in chunks of 1000 OM SKUs\n",
    "    for chunk in chunks:\n",
    "        unique_skus_str = ', '.join([f\"'{sku}'\" for sku in chunk[f'{scorecard_date} OM SKU']])\n",
    "\n",
    "        query_edw = f\"\"\"\n",
    "        SELECT\n",
    "            pd.ID_SKU AS \"{scorecard_date} OM SKU\", pd.NM_SKU AS \"OM Item Description\",\n",
    "            (pd.AT_CORP_BASE_COST * pd.RT_PURCHASE_UOM_CONVERSION) AS \"$ PUOM Unit Cost\",\n",
    "            pd.CD_BASE_UOM AS \"Base UOM\", pd.DS_BASE_UOM_FACTOR AS \"Base UOM Factor\",\n",
    "            pd.CD_PURCHASE_UOM AS \"Purchase UOM\", pd.RT_PURCHASE_UOM_CONVERSION AS \"Purchase UOM Factor\",\n",
    "            pd.CD_ALTERNATE_UOM_1 AS \"Alt UOM 1\", pd.RT_ALTERNATE_UOM_CONV_1 AS \"Alt UOM 1 Factor\",\n",
    "            pd.CD_ALTERNATE_UOM_2 AS \"Alt UOM 2\", pd.RT_ALTERNATE_UOM_CONV_2 AS \"Alt UOM 2 Factor\",\n",
    "            pd.CD_ALTERNATE_UOM_3 AS \"Alt UOM 3\", pd.RT_ALTERNATE_UOM_CONV_3 AS \"Alt UOM 3 Factor\",\n",
    "            pd.CD_ALTERNATE_UOM_4 AS \"Alt UOM 4\", pd.RT_ALTERNATE_UOM_CONV_4 AS \"Alt UOM 4 Factor\",\n",
    "            pd.CD_ALTERNATE_UOM_5 AS \"Alt UOM 5\", pd.RT_ALTERNATE_UOM_CONV_5 AS \"Alt UOM 5 Factor\",\n",
    "            pd.CD_ALTERNATE_UOM_6 AS \"Alt UOM 6\", pd.RT_ALTERNATE_UOM_CONV_6 AS \"Alt UOM 6 Factor\"\n",
    "        FROM DBINSTANCE.PRODUCT_DIM pd\n",
    "        WHERE pd.ID_SKU IN ({unique_skus_str})\n",
    "        AND pd.FG_CURRENT = 'Y'\n",
    "        AND pd.FG_ACTIVE_SKU = 'ACTIVE'\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            cur.execute(query_edw)\n",
    "            rows = cur.fetchall()\n",
    "            column_names = [column[0] for column in cur.description]\n",
    "            results_edw.append(pd.DataFrame(rows, columns=column_names))\n",
    "        except Exception as e:\n",
    "            print(f\"Error during EDW query execution: {e}\")\n",
    "\n",
    "    # Concatenate all chunks into one DataFrame\n",
    "    df_edw = pd.concat(results_edw, ignore_index=True)\n",
    "\n",
    "# Remove duplicate OM SKUs from the EDW dataframe\n",
    "df_edw = df_edw.drop_duplicates(subset=f'{scorecard_date} OM SKU', keep='first')\n",
    "\n",
    "# Remove 'na' data from scorecard UOM Conversion Factor data and replace 'u' and 0 with NULL\n",
    "alt_cols = [col for col in df_edw.columns if col.startswith('Alt')]\n",
    "df_edw[alt_cols] = df_edw[alt_cols].apply(replace_values)\n",
    "\n",
    "# Format any columns that end with the word \"Factor\" into a float\n",
    "factor_cols = [col for col in df_edw.columns if col.endswith('Factor')]\n",
    "df_edw[factor_cols] = df_edw[factor_cols].astype(float)\n",
    "\n",
    "# Check for OM SKUs not found in the EDW UOM conversion data pull\n",
    "missing_records = df_unique_ready_release_om_skus[~df_unique_ready_release_om_skus[f'{scorecard_date} OM SKU'].isin(df_edw[f'{scorecard_date} OM SKU'])]\n",
    "print(\"OM SKUs not found in PRODUCT_DIM table: \", missing_records)\n",
    "\n",
    "# Output the number of SKUs from the data pull\n",
    "print(\"# of SKUs from data pull: \", len(df_edw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 rows where there are several OM SKUs to one single HELPERKEY.\n",
      "There are 0 PXR records that are marked YES ready release for ATS and do not have an OM SKU attached.\n",
      "There are 0 records that are an exact match or approved sub, but are marked NO ready release for ATS.\n",
      "There are 13 records that are NOT an exact match or approved sub, but are marked YES ready release for ATS.\n",
      "There are 0 records that did not match to scorecard data.\n"
     ]
    }
   ],
   "source": [
    "### Validate PXR scorecard & usage data\n",
    "\n",
    "# Make a copy of raw usage data\n",
    "df_usage = data_usage.copy()\n",
    "\n",
    "# Change datatype of HELPERKEY and make it uppercase\n",
    "df_usage['HELPERKEY'] = df_usage['HELPERKEY'].astype(str).str.upper()\n",
    "\n",
    "# Check and convert 'ORDER DATE' to datetime format\n",
    "df_usage['ORDER DATE'] = pd.to_datetime(df_usage['ORDER DATE'])\n",
    "df_usage['ORDER YEAR-Mth'] = df_usage['ORDER DATE'].dt.strftime('%Y-%m')\n",
    "\n",
    "# Group by 'HELPERKEY' and count unique 'OM SKU' values for one-to-many relationships\n",
    "df_helperkey_to_multiple_om_skus = (\n",
    "    df_scorecard.groupby('HELPERKEY')[f'{scorecard_date} OM SKU'].nunique()\n",
    "    .loc[lambda x: x > 1].reset_index()\n",
    ")\n",
    "\n",
    "# Rows where READY RELEASE ATS == 'YES' but no OM SKU\n",
    "df_ready_release_yes_no_om_sku = df_scorecard[\n",
    "    (df_scorecard[f'{scorecard_date} READY RELEASE ATS'] == 'YES') &\n",
    "    (df_scorecard[f'{scorecard_date} OM SKU'].isnull() | (df_scorecard[f'{scorecard_date} OM SKU'] == 0))\n",
    "]\n",
    "\n",
    "# Rows where READY RELEASE ATS == 'no' but SCORECARD CATEGORY contains 'EXACT MATCH' or 'APPROVED'\n",
    "df_no_ready_release_exact_match_approved = df_scorecard[\n",
    "    (df_scorecard[f'{scorecard_date} READY RELEASE ATS'] == 'no') &\n",
    "    df_scorecard[f'{scorecard_date} SCORECARD CATEGORY'].str.contains('EXACT MATCH|APPROVED', case=False, na=False)\n",
    "]\n",
    "\n",
    "# Rows where READY RELEASE ATS == 'YES' but NOT an exact match or approved sub\n",
    "df_ready_release_yes_not_exact_match_approved = df_scorecard[\n",
    "    (df_scorecard[f'{scorecard_date} READY RELEASE ATS'] == 'YES') &\n",
    "    ~df_scorecard[f'{scorecard_date} SCORECARD CATEGORY'].str.contains('EXACT MATCH|APPROVED', case=False, na=False)\n",
    "]\n",
    "\n",
    "# Look for records in usage data that did not match to scorecard data\n",
    "df_nomatchkey = pd.merge(df_usage, df_scorecard, on='HELPERKEY', how='outer', indicator=True)\n",
    "df_nomatchkey = df_nomatchkey[df_nomatchkey['_merge'] == 'left_only']\n",
    "\n",
    "# Display validation results\n",
    "print(f'There are {len(df_helperkey_to_multiple_om_skus)} rows where there are several OM SKUs to one single HELPERKEY.')\n",
    "print(f'There are {len(df_ready_release_yes_no_om_sku)} PXR records that are marked YES ready release for ATS and do not have an OM SKU attached.')\n",
    "print(f'There are {len(df_no_ready_release_exact_match_approved)} records that are an exact match or approved sub, but are marked NO ready release for ATS.')\n",
    "print(f'There are {len(df_ready_release_yes_not_exact_match_approved)} records that are NOT an exact match or approved sub, but are marked YES ready release for ATS.')\n",
    "print(f'There are {len(df_nomatchkey)} records that did not match to scorecard data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usage1 = df_usage.copy()\n",
    "\n",
    "# Merge with DC and Account data\n",
    "if ATS_workflow_type == 'initial':\n",
    "    df_usage1 = df_usage1.merge(df_metadata, on='BUSN Unit', how='left')\n",
    "\n",
    "# Merge scorecard data with EDW data\n",
    "df_scorecard = df_scorecard.merge(df_edw, on=f'{scorecard_date} OM SKU', how='left')\n",
    "\n",
    "if ATS_workflow_type == 'initial':\n",
    "    # Capture column headers from raw usage data\n",
    "    rawusagecolumns = df_usage1.columns\n",
    "\n",
    "    # Merge scorecard data with usage data\n",
    "    df_usagewithscorecard = df_usage1.merge(df_scorecard, on='HELPERKEY', how='left')\n",
    "\n",
    "elif ATS_workflow_type == 'update':\n",
    "    # Remove previous weeks' metadata, then capture column headers from raw usage data\n",
    "    rawusagecolumns = list(df_usage1.columns[:-20])  # Explicit column list for clarity\n",
    "\n",
    "    # Bring in new UOM/EDW data\n",
    "    df_usagewithscorecard = pd.merge(df_usage1[rawusagecolumns], df_scorecard, on='HELPERKEY', how='left')\n",
    "\n",
    "# Convert ORDER QTYs @ OM SKU's Purchase UOM\n",
    "def calculate_converted_qty(df):\n",
    "    num_alt_uoms = 6  # Change this to match the number of Alt UOMs available\n",
    "    uom_columns = ['Purchase UOM', 'Base UOM'] + [f'Alt UOM {i}' for i in range(1, num_alt_uoms + 1)]\n",
    "    factors = ['Purchase UOM Factor', 'Base UOM Factor'] + [f'Alt UOM {i} Factor' for i in range(1, num_alt_uoms + 1)]\n",
    "\n",
    "    # Initialize ORDER QTY CONVERTED column\n",
    "    df['ORDER QTY CONVERTED'] = 0.0\n",
    "\n",
    "    for uom, factor in zip(uom_columns, factors):\n",
    "        if uom in df.columns and factor in df.columns:\n",
    "            mask = df['ORDER UOM'] == df[uom]\n",
    "            if uom == 'Purchase UOM':\n",
    "                df.loc[mask, 'ORDER QTY CONVERTED'] = df.loc[mask, 'ORDER QTY']\n",
    "            elif uom == 'Base UOM':\n",
    "                df.loc[mask, 'ORDER QTY CONVERTED'] = df.loc[mask, 'ORDER QTY'] / df.loc[mask, 'Purchase UOM Factor']\n",
    "            else:\n",
    "                df.loc[mask, 'ORDER QTY CONVERTED'] = (df.loc[mask, 'ORDER QTY'] * df.loc[mask, factor] / df.loc[mask, 'Purchase UOM Factor'])\n",
    "\n",
    "    # Create conversion error notes\n",
    "    df['Conversion Error Note'] = df['ORDER QTY CONVERTED'].isna().replace({True: \"NO UOM Match\", False: \"\"})\n",
    "    return df\n",
    "\n",
    "# Apply conversion function to the usage data\n",
    "df_usage_converted = calculate_converted_qty(df_usagewithscorecard.copy())\n",
    "\n",
    "# Create dataframe to highlight UOM conversion issues\n",
    "df_conversionerror = (\n",
    "    df_usage_converted.loc[df_usage_converted['Conversion Error Note'].eq('NO UOM Match'), \n",
    "                           ['HELPERKEY', 'MMIS ID', f'{scorecard_date} OM SKU', 'ORDER UOM']]\n",
    "    .drop_duplicates()\n",
    ")\n",
    "\n",
    "# Create a function that categorizes each row by what changes happened between current and previous week's scorecards and input the category into a new column 'PXR Update Category'\n",
    "def update_pxr_category(df, scorecard_date_prev, scorecard_date):\n",
    "    # Initialize 'PXR Update Category' column with empty strings\n",
    "    df['PXR Update Category'] = ''\n",
    "\n",
    "    # Condition 1: 'YES TO NO: ATS Removes'\n",
    "    condition1 = (df[f'{scorecard_date_prev} READY RELEASE ATS'] == 'YES') & (df[f'{scorecard_date} READY RELEASE ATS'] == 'NO')\n",
    "    df.loc[condition1, 'PXR Update Category'] = 'YES TO NO: ATS Removes'\n",
    "\n",
    "    # Condition 2: 'SKU CHANGE'\n",
    "    condition2 = (\n",
    "        (df[f'{scorecard_date_prev} OM SKU'] != df[f'{scorecard_date} OM SKU']) &\n",
    "        df[f'{scorecard_date_prev} OM SKU'].notnull() &\n",
    "        (df[f'{scorecard_date_prev} READY RELEASE ATS'] == 'YES') & \n",
    "        (df[f'{scorecard_date} READY RELEASE ATS'] == 'YES')\n",
    "    )\n",
    "    df.loc[condition2, 'PXR Update Category'] = 'SKU CHANGE'\n",
    "\n",
    "    # Condition 3: 'NO TO YES: NEW ADDS'\n",
    "    condition3 = (df[f'{scorecard_date_prev} READY RELEASE ATS'] == 'NO') & (df[f'{scorecard_date} READY RELEASE ATS'] == 'YES')\n",
    "    df.loc[condition3, 'PXR Update Category'] = 'NO TO YES: NEW ADDS'\n",
    "\n",
    "    return df\n",
    "\n",
    "# Merge new scorecard updates into initial usage data w/ PXR scorecard data from Initial ATS File\n",
    "if ATS_workflow_type == 'initial':\n",
    "    # Filter for Y ready release items, filter out items with 0 usage or UOM conversion issues\n",
    "    df_usage_converted_YesATS = df_usage_converted[\n",
    "        (df_usage_converted['Conversion Error Note'] == '') &\n",
    "        (df_usage_converted[f'{scorecard_date} READY RELEASE ATS'] == 'YES')\n",
    "    ]\n",
    "\n",
    "elif ATS_workflow_type == 'update':\n",
    "    # Ensure update_pxr_category function is modifying the dataframe as expected\n",
    "    df_usage_converted = update_pxr_category(df_usage_converted, scorecard_date_prev, scorecard_date)\n",
    "\n",
    "    # Filter for Y ready release items with PXR Update Category conditions\n",
    "    df_usage_converted_YesATS = df_usage_converted[\n",
    "        (df_usage_converted['Conversion Error Note'] == '') &\n",
    "        (df_usage_converted['PXR Update Category'].isin(['NO TO YES: NEW ADDS', 'SKU CHANGE']))\n",
    "    ]\n",
    "\n",
    "    # Create pivot of removes that may need to be done\n",
    "    df_updates_removes = (df_usage_converted[df_usage_converted['PXR Update Category'].isin(['YES TO NO: ATS Removes', 'SKU CHANGE'])]\n",
    "                                   .groupby(['LOC', 'OM ACCOUNT', f'{scorecard_date_prev} OM SKU', f'{scorecard_date} OM SKU', f'{scorecard_date} SCORECARD CATEGORY', f'{scorecard_date} ADD/CHANGE DATE (ATS)'])\n",
    "                                    .size()\n",
    "                                    .reset_index(name='Count'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pivot PO data into a monthly format\n",
    "def create_pivot(df, index_cols):\n",
    "    pivot_df = df.pivot_table(index=index_cols, columns='ORDER YEAR-Mth', values='ORDER QTY CONVERTED', aggfunc=\"sum\").reset_index()\n",
    "    return pivot_df\n",
    "\n",
    "# Use function to create pivoted dataframes for both DC and Account level roll-ups\n",
    "df_ATS = create_pivot(df_usage_converted_YesATS, ['LOC', 'OM ACCOUNT', f'{scorecard_date} OM SKU', 'Purchase UOM', '$ PUOM Unit Cost', 'OM Item Description'])\n",
    "df_DClevelusagerules = create_pivot(df_usage_converted_YesATS, ['LOC', f'{scorecard_date} OM SKU', 'Purchase UOM'])\n",
    "\n",
    "# Create a list of header names for ORDER QTY column(s) to call on them in below functions\n",
    "date_columns_list = df_ATS.columns[6:]\n",
    "\n",
    "### Create dataframe to capture MMIS IDs and line counts\n",
    "# Step 1: Count occurrences for each MMIS ID\n",
    "df_MMISIDs = (\n",
    "    df_usage_converted_YesATS\n",
    "    .groupby(['LOC', 'OM ACCOUNT', f'{scorecard_date} OM SKU', 'MMIS ID'])\n",
    "    .size()\n",
    "    .reset_index(name='Count')  # Store the counts in a new column\n",
    ")\n",
    "\n",
    "# Step 2: Sum counts regardless of MMIS ID for each group\n",
    "summed_counts = (\n",
    "    df_MMISIDs.groupby(['LOC', 'OM ACCOUNT', f'{scorecard_date} OM SKU'])\n",
    "    .agg(Total_Count=('Count', 'sum'))  # Sum counts\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Step 3: Identify the MMIS ID with the highest count for each group\n",
    "highest_mmis = (\n",
    "    df_MMISIDs.loc[df_MMISIDs.groupby(['LOC', 'OM ACCOUNT', f'{scorecard_date} OM SKU'])['Count'].idxmax()]\n",
    "    .reset_index(drop=True)  # Reset index for cleanliness\n",
    ")\n",
    "\n",
    "# Step 4: Merge summed counts with the MMIS ID with the highest count\n",
    "final_linecounts_MMISIDs = (\n",
    "    pd.merge(summed_counts, highest_mmis[['LOC', 'OM ACCOUNT', f'{scorecard_date} OM SKU', 'MMIS ID']],\n",
    "             on=['LOC', 'OM ACCOUNT', f'{scorecard_date} OM SKU'], how='left')\n",
    ")\n",
    "\n",
    "# Calculate the average monthly line count\n",
    "# final_counts will have the structure you need, including summed counts and the MMIS ID with the highest count\n",
    "final_linecounts_MMISIDs['Avg Monthly Line Count'] = (final_linecounts_MMISIDs['Total_Count'] / numberofmonths).round(1)\n",
    "\n",
    "# Bring in line counts, MMIS IDs, and Blue Yonder metadata into main ATS dataframe\n",
    "df_ATS = df_ATS.merge(final_linecounts_MMISIDs, on=['LOC', 'OM ACCOUNT', f'{scorecard_date} OM SKU'], how='left')\n",
    "df_ATS = pd.merge(df_ATS, df_by, on=['LOC', f'{scorecard_date} OM SKU'], how='left')\n",
    "\n",
    "# PO usage format: Calculate statistics around usage trends\n",
    "def calculate_POstatistics(df, date_columns_list):\n",
    "    df['Count Last 6mths w/ Activity'] = df[date_columns_list[-6:]].count(axis=1)\n",
    "    df['Avg Order Qty Last 6mth'] = df[date_columns_list[-6:]].mean(axis=1)\n",
    "    df['Avg Order Qty Last 3mth'] = df[date_columns_list[-3:]].mean(axis=1)\n",
    "    df['Avg Order Qty Last 6mth 75%'] = df['Avg Order Qty Last 6mth'] * 0.75\n",
    "    df['Sum Order Qty Last 6mth'] = df[date_columns_list[-6:]].sum(axis=1)\n",
    "    df['Standard Deviation Last 6mth'] = df[date_columns_list[-6:]].std(axis=1)\n",
    "    df['Variation in Usage Trend (CoV)'] = df['Standard Deviation Last 6mth'] / df['Avg Order Qty Last 6mth']\n",
    "    df['Activity Last 6mth?'] = np.where(df[date_columns_list[-6:]].sum(axis=1) > 0, 'Y', 'N')\n",
    "    df['Activity Last 4mth?'] = np.where(df[date_columns_list[-4:]].sum(axis=1) > 0, 'Y', 'N')\n",
    "    df['Activity Last 3mth?'] = np.where(df[date_columns_list[-3:]].sum(axis=1) > 0, 'Y', 'N')\n",
    "    df['Sum Qty First 3mth'] = df[date_columns_list[-6:-3]].sum(axis=1)\n",
    "    df['Sum Qty Last 3mth'] = df[date_columns_list[-3:]].sum(axis=1)\n",
    "    df['Usage Trend'] = np.where(df['Sum Qty Last 3mth'] > df['Sum Qty First 3mth'], 'Up', 'Down')\n",
    "    return df\n",
    "\n",
    "# PO usage format: ATS rules determinations\n",
    "def dc_level_POusagerules(row):\n",
    "    if row['Activity Last 6mth?'] == 'N':\n",
    "        return 'Rule 1a'\n",
    "    if row['Activity Last 4mth?'] == 'N':\n",
    "        return 'Rule 1b'\n",
    "    if row['Activity Last 3mth?'] == 'N':\n",
    "        return 'Rule 1c'\n",
    "    if row['Count Last 6mths w/ Activity'] >= 4:\n",
    "        return 'Rule 3a' if row['Usage Trend'] == 'Up' else 'Rule 3b'\n",
    "    if row['Count Last 6mths w/ Activity'] in [2, 3]:\n",
    "        return 'Rule 4a'\n",
    "    if row['Count Last 6mths w/ Activity'] == 1:\n",
    "        return 'Rule 5a'\n",
    "\n",
    "# Monthly usage format: ATS rules determination\n",
    "def dc_level_mthlyusagerules(row, date_columns_list):\n",
    "    total_sum = row[date_columns_list].sum()/(numberofmonths)\n",
    "    if total_sum >= 0.25:\n",
    "        return 'Y ATS: Stock item'\n",
    "    if total_sum == 0:\n",
    "        return 'N ATS: Sell as nonstock, no usage'\n",
    "    if total_sum < 0.25:\n",
    "        return 'N ATS: Sell as nonstock, below stocking criteria'\n",
    "\n",
    "# PO usage format: ATS qty calculation logic\n",
    "def calculate_ATS_qty_PO(row, date_columns_list):\n",
    "    if row['ATS DC Level Usage Rule'] == 'Rule 1a' or row['ATS DC Level Usage Rule'] == 'Rule 1b':\n",
    "        return 0\n",
    "    elif row['ATS DC Level Usage Rule'] == 'Rule 1c':\n",
    "        value = row[date_columns_list].sum() / 6\n",
    "        return (round(value, 1) if value > 2 else round(value, 2))\n",
    "    elif row['ATS DC Level Usage Rule'] == 'Rule 3a':\n",
    "        value = row['Avg Order Qty Last 3mth'] if row['Sum Qty Last 3mth'] > 0 else row['Avg Order Qty Last 6mth']\n",
    "        return (math.floor(value) if value > 2 else round(value, 2))\n",
    "    elif row['ATS DC Level Usage Rule'] == 'Rule 3b':\n",
    "        value = row['Avg Order Qty Last 6mth']\n",
    "        return (math.floor(value) if value > 2 else round(value, 2))\n",
    "    elif row['ATS DC Level Usage Rule'] == 'Rule 4a':\n",
    "        value = row['Avg Order Qty Last 6mth 75%']\n",
    "        return (math.floor(value) if value > 2 else round(value, 2))\n",
    "    elif row['ATS DC Level Usage Rule'] == 'Rule 5a':\n",
    "        value = row['Sum Order Qty Last 6mth'] / 6\n",
    "        return (round(value, 1) if value > 2 else round(value, 2))\n",
    "    return np.nan\n",
    "\n",
    "# Monthly usage format: ATS qty calculation logic (Simple average)\n",
    "def calculate_ATS_qty_Mthly(df, date_columns_list):\n",
    "    df['QTY ATS TO LOAD @ PUOM'] = df[date_columns_list].sum(axis=1)/(numberofmonths)\n",
    "    return df\n",
    "\n",
    "# Main ATS determination and calculation processed\n",
    "if usageformat == 'PO':\n",
    "    # 1. Calculate statistics for both DC and Account level\n",
    "    df_DClevelusagerules = calculate_POstatistics(df_DClevelusagerules, date_columns_list)\n",
    "    df_ATS = calculate_POstatistics(df_ATS, date_columns_list)\n",
    "\n",
    "    # 2. Determine usage rules for ATS qty calculations at DC level\n",
    "    df_DClevelusagerules['ATS DC Level Usage Rule'] = df_DClevelusagerules.apply(dc_level_POusagerules, axis=1)\n",
    "\n",
    "    # 3. Merge DC-level rules into account-level data\n",
    "    df_ATS = df_ATS.merge(df_DClevelusagerules[['LOC', f'{scorecard_date} OM SKU', 'ATS DC Level Usage Rule']],\n",
    "                          on=['LOC', f'{scorecard_date} OM SKU'], how='left')\n",
    "\n",
    "    # 4. Calculate ATS qtys for PO data\n",
    "    df_ATS['QTY ATS TO LOAD @ PUOM'] = df_ATS.apply(lambda row: calculate_ATS_qty_PO(row, date_columns_list), axis=1)\n",
    "\n",
    "    # 5. Create column ATS Build Category that buckets based on ATS determinations\n",
    "    df_ATS['ATS Build Category'] = np.select(\n",
    "        [df_ATS['Activity Last 6mth?'] == 'N',\n",
    "        df_ATS['Activity Last 4mth?'] == 'N',\n",
    "        df_ATS['QTY ATS TO LOAD @ PUOM'] < 0.25,\n",
    "        df_ATS['QTY ATS TO LOAD @ PUOM'] >= 0.25],\n",
    "        ['N ATS: No usage last 4+ mths',\n",
    "        'N ATS: No usage last 4+ mths',\n",
    "        'N ATS: usage below stocking criteria',\n",
    "        'Y ATS: Stock Item'],\n",
    "        default=''\n",
    "    )\n",
    "\n",
    "elif usageformat == 'monthly':\n",
    "    # 1. Determine ATS status at DC level based on summed monthly data\n",
    "    df_DClevelusagerules['ATS Build Category'] = df_DClevelusagerules.apply(lambda row: dc_level_mthlyusagerules(row, date_columns_list), axis=1)\n",
    "\n",
    "    # 2. Merge DC-level usage rules into account-level data\n",
    "    df_ATS = df_ATS.merge(df_DClevelusagerules[['LOC', f'{scorecard_date} OM SKU', 'ATS Build Category']],\n",
    "                          on=['LOC', f'{scorecard_date} OM SKU'], how='left')\n",
    "\n",
    "    # 3. Calculate ATS qtys for monthly data (simple sum)\n",
    "    df_ATS = calculate_ATS_qty_Mthly(df_ATS, date_columns_list)\n",
    "\n",
    "# Calculate estimated ATS Costs using PUOM Unit Costs\n",
    "df_ATS['$ Estimated ATS'] = df_ATS['QTY ATS TO LOAD @ PUOM'] * df_ATS['$ PUOM Unit Cost']\n",
    "df_ATS = df_ATS.sort_values(by='$ Estimated ATS', ascending=False)\n",
    "\n",
    "# Function to fill any N ATS records with the initial ATS load\n",
    "def initial_atsscorecardbuildcategory(row):\n",
    "    if pd.isnull(row['ATS Build Category']):\n",
    "        if row['Conversion Error Note'] == 'NO UOM Match':\n",
    "            return 'N ATS: UOM Conversion Error'\n",
    "        elif row[f'{scorecard_date} READY RELEASE ATS'] != 'YES':\n",
    "            return 'N ATS: N ready release for ATS'\n",
    "    return row['ATS Build Category']\n",
    "\n",
    "if ATS_workflow_type == 'initial':\n",
    "    # Merge ATS Build Category data from df_ATS\n",
    "    df_usage_converted = df_usage_converted.merge(\n",
    "        df_ATS[['OM ACCOUNT', f'{scorecard_date} OM SKU', 'ATS Build Category']],\n",
    "        on=['OM ACCOUNT', f'{scorecard_date} OM SKU'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Fill ats Build Category for N ready release items\n",
    "    df_usage_converted['ATS Build Category'] = df_usage_converted.apply(initial_atsscorecardbuildcategory, axis=1)\n",
    "\n",
    "elif ATS_workflow_type == 'update':\n",
    "    # Merge to get new ATS Build Category data from df_ATS\n",
    "    df_usage_converted = df_usage_converted.merge(\n",
    "        df_ATS[['OM ACCOUNT', f'{scorecard_date} OM SKU', 'ATS Build Category']],\n",
    "        on=['OM ACCOUNT', f'{scorecard_date} OM SKU'], \n",
    "        how='left',\n",
    "        suffixes=('', '_new')  # Adding suffix to avoid overwriting\n",
    "    )\n",
    "    \n",
    "    # Update ATS Build Category only where df_ATS has a non-null value\n",
    "    df_usage_converted['ATS Build Category'] = np.where(\n",
    "        # First condition: use new ATS Build Category if available\n",
    "        df_usage_converted['ATS Build Category_new'].notna(),\n",
    "        df_usage_converted['ATS Build Category_new'],\n",
    "        # Second condition: if last week's READY RELEASE ATS is 'YES' and current is 'NO'\n",
    "        np.where(\n",
    "            (df_usage_converted[f'{scorecard_date_prev} READY RELEASE ATS'] == 'YES') &\n",
    "            (df_usage_converted[f'{scorecard_date} READY RELEASE ATS'] == 'NO'),\n",
    "            'N ATS: N ready release for ATS',\n",
    "            # Default to existing ATS Build Category if neither condition is met\n",
    "            df_usage_converted['ATS Build Category']\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Drop the temporary column used for merging\n",
    "    df_usage_converted.drop(columns=['ATS Build Category_new'], inplace=True)\n",
    "\n",
    "# Reorder columns\n",
    "df_ATS = df_ATS.reindex(columns=['LOC', f'{scorecard_date} OM SKU', 'QTY ATS TO LOAD @ PUOM', 'Purchase UOM', 'OM ACCOUNT'\n",
    "                                 , 'MMIS ID', 'Avg Monthly Line Count', 'ATS Build Category', '$ Estimated ATS'\n",
    "                                 , '$ PUOM Unit Cost', 'OM Item Description', 'ATS DC Level Usage Rule', 'DC 4wk fcst', 'Days Lead Time', 'IFLAG'\n",
    "                                 , *date_columns_list # This code unpacks the list of date columns\n",
    "                                 , 'Count Last 6mths w/ Activity', 'Avg Order Qty Last 6mth', 'Avg Order Qty Last 3mth', 'Avg Order Qty Last 6mth 75%', 'Sum Order Qty Last 6mth'\n",
    "                                 , 'Standard Deviation Last 6mth', 'Variation in Usage Trend (CoV)', 'Activity Last 6mth?', 'Activity Last 4mth?', 'Activity Last 3mth?'])\n",
    "\n",
    "df_usage_converted = df_usage_converted.reindex(columns=[*rawusagecolumns[:-2] # This removes ATS Build Category from the reindex so it isn't brought in twice\n",
    "                                                        , *(['LOC', 'OM ACCOUNT'] if ATS_workflow_type == 'initial' else [])\n",
    "                                                        , f'{scorecard_date} READY RELEASE ATS'\n",
    "                                                        , f'{scorecard_date} ADD/CHANGE DATE (ATS)', f'{scorecard_date} SCORECARD CATEGORY'\n",
    "                                                        , f'{scorecard_date} OM SKU'\n",
    "                                                        , 'PXR Update Category'\n",
    "                                                        , 'ATS Build Category', 'Conversion Error Note'\n",
    "                                                        , 'ORDER QTY CONVERTED', 'OM Item Description', '$ PUOM Unit Cost'\n",
    "                                                        , 'Base UOM', 'Base UOM Factor'\n",
    "                                                        , 'Purchase UOM', 'Purchase UOM Factor', 'Alt UOM 1', 'Alt UOM 1 Factor'\n",
    "                                                        , 'Alt UOM 2', 'Alt UOM 2 Factor', 'Alt UOM 3', 'Alt UOM 3 Factor'\n",
    "                                                        , 'Alt UOM 4', 'Alt UOM 4 Factor', 'Alt UOM 5', 'Alt UOM 5 Factor'\n",
    "                                                        , 'Alt UOM 6', 'Alt UOM 6 Factor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Excel writer instance\n",
    "file_path = f\"{projectoutputs_folder}/{customer_name}_ATSFile_{ATS_workflow_type}_{scorecard_date}.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(file_path, engine='xlsxwriter') as writer:\n",
    "    # Dictionary mapping of sheet names to dataframes\n",
    "    sheets = {\n",
    "        'ATS Build': df_ATS,\n",
    "        'Raw Usage': df_usage_converted,\n",
    "        'UOM Conversion Errors': df_conversionerror,\n",
    "         **({'ATS Removes': df_updates_removes} if ATS_workflow_type == 'update' else {})\n",
    "    }\n",
    "\n",
    "    # Write dataframes to their respective sheets in a single pass\n",
    "    for sheet_name, df in sheets.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    # Get the xlsxwriter workbook object and define formats\n",
    "    workbook = writer.book\n",
    "    header_format = workbook.add_format({\n",
    "        'bold': True,\n",
    "        'text_wrap': True,\n",
    "        'valign': 'center',\n",
    "        'align': 'center',\n",
    "        'bg_color': '#D9EAD3'\n",
    "    })\n",
    "    currency_format = workbook.add_format({'num_format': '$#,##0.00'})\n",
    "\n",
    "    # Function to format a worksheet based on the dataframe columns\n",
    "    def format_worksheet(worksheet, df):\n",
    "        if df.empty:\n",
    "            return  # Exit if the DataFrame is empty, do not format\n",
    "\n",
    "        for col_num, value in enumerate(df.columns):\n",
    "            worksheet.write(0, col_num, value, header_format)  # Apply header format\n",
    "\n",
    "            # Calculate column width only if the DataFrame has rows\n",
    "            if not df[value].empty:\n",
    "                column_width = max(len(str(val)) for val in df[value].values) + 2  # Dynamically set column width based on content\n",
    "                worksheet.set_column(col_num, col_num, column_width)  # Adjust column width based on data content\n",
    "\n",
    "                # Apply currency format if column header starts with $\n",
    "                if value.startswith('$'):\n",
    "                    worksheet.set_column(col_num, col_num, None, currency_format)\n",
    "\n",
    "        worksheet.autofilter(0, 0, 0, len(df.columns) - 1)  # Add autofilter to header\n",
    "        worksheet.freeze_panes(1, 0)  # Freeze top row\n",
    "\n",
    "    # Apply formatting to each worksheet after data is written\n",
    "    for sheet_name, df in sheets.items():\n",
    "        format_worksheet(writer.sheets[sheet_name], df)\n",
    "\n",
    "\n",
    "    # Apply formatting to each worksheet after data is written\n",
    "    for sheet_name, df in sheets.items():\n",
    "        format_worksheet(writer.sheets[sheet_name], df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
