{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import oracledb\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress all Python warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set pandas option to display numbers in plain format\n",
    "pd.set_option('display.float_format', '{:.0f}'.format)\n",
    "\n",
    "# Get today's date\n",
    "today = datetime.today()\n",
    "\n",
    "# Calculate the number of days between today and the most recent Sunday\n",
    "newestsunday = (today.weekday() - 6) % 7\n",
    "\n",
    "# Calculate the number of days between today and X Sundays ago\n",
    "oldestsunday = newestsunday + (7 * 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales Facts # of records returned:  1249385\n",
      "Blue Yonder # of records returned:  6038735\n"
     ]
    }
   ],
   "source": [
    "# Establish the database connection\n",
    "connection = oracledb.connect(user='usernamehere',\n",
    "                              password='pswdhere',\n",
    "                              dsn=oracledb.makedsn('hostnamehere', '1234', service_name='DATABASE_NAME_HERE'))\n",
    "cursor = connection.cursor()\n",
    "\n",
    "query_edw = f\"\"\"\n",
    "SELECT od.ID_DIVISION\n",
    "\t,pd.ID_SKU\n",
    "\t,pgd.CD_HPIS_MAJOR || ' - ' || pgd.DS_HPIS_MAJOR AS \"Major HPIS Class - Description\"\n",
    "\t,(pd.AT_CORP_BASE_COST * pd.RT_PURCHASE_UOM_CONVERSION) AS \"Unit_Cost_PUOM\"\n",
    "    ,SUM(sf.QT_ORDER / pd.RT_PURCHASE_UOM_CONVERSION) AS ORDERED_QTY_PUOM\n",
    "    ,SUM(sf.QT_SHIP / pd.RT_PURCHASE_UOM_CONVERSION) AS SHIPPED_QTY_PUOM\n",
    "\t,CASE \n",
    "\t\tWHEN TO_CHAR(td.KY_TIME, 'D') = 1\n",
    "\t\t\tTHEN TRUNC(td.KY_TIME)\n",
    "\t\tELSE TRUNC(td.KY_TIME, 'IW') - 1\n",
    "\t\tEND AS STARTDATE\n",
    "FROM DBINSTANCE.SALES_FACTS sf\n",
    "JOIN DBINSTANCE.PRODUCT_DIM pd ON pd.KY_PRODUCT = sf.KY_PRODUCT\n",
    "JOIN DBINSTANCE.ORG_DIM od ON sf.KY_ORG = od.KY_ORG\n",
    "JOIN DBINSTANCE.TIME_DIM td ON sf.KY_TIME = td.KY_TIME\n",
    "JOIN DBINSTANCE.INVOICE_TYPE_DIM itd ON itd.KY_INVOICE_TYPE = sf.KY_INVOICE_TYPE\n",
    "LEFT JOIN DBINSTANCE.PRODUCT_GROUP_DIM pgd ON pd.CD_HPIS = pgd.CD_HPIS_CLASS\n",
    "WHERE sf.QT_ORDER > 0\n",
    "\tAND td.KY_TIME BETWEEN TRUNC(SYSDATE) - {oldestsunday}\n",
    "\t\tAND TRUNC(SYSDATE) - {newestsunday} \t\t-- Pulls 60 days worth of data\n",
    "\tAND pd.ID_SKU NOT LIKE '8%' \t\t\t-- Excludes customer owned inventory\n",
    "\tAND itd.CD_INVOICE_TYPE <> 'REBILL'\n",
    "\tAND itd.CD_INVOICE_TYPE <> 'DIRECT' \t-- Filters out direct orders (supplier directly to customer)\n",
    "\tAND itd.FG_ORIGINAL_ORDER = 'ORIGINAL' \t-- Filters out backorders\n",
    "\tAND pd.FG_CURRENT = 'Y'\n",
    "\tAND pd.FG_ACTIVE_SKU = 'ACTIVE'\n",
    "    AND od.ID_DIVISION IN (03, 08, 14, 16, 20, 21, 30, 37, 41, 44, 45, 48, 49, 50, 51, 53, 56, 58, 59, 60, 64, 65, 66, 67, 68, 69, 70, 71, 78, 80, 82, 84, 85, 87, 89, 90, 91, 92, 93, 94, 96, 98)\n",
    "    GROUP BY od.ID_DIVISION, ID_SKU, pgd.CD_HPIS_MAJOR || ' - ' || pgd.DS_HPIS_MAJOR, (pd.AT_CORP_BASE_COST * pd.RT_PURCHASE_UOM_CONVERSION),\n",
    "         CASE\n",
    "             WHEN TO_CHAR(td.KY_TIME, 'D') = 1 THEN TRUNC(td.KY_TIME)\n",
    "             ELSE TRUNC(td.KY_TIME, 'IW') - 1\n",
    "         END\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "query_by = f\"\"\"\n",
    "SELECT SUBSTR(s.loc, LENGTH(s.loc) - 1, 2) AS \"ID_DIVISION\",\n",
    "   i.VENDORNUM,\n",
    "   hf.DMDUNIT AS \"ID_SKU\",\n",
    "   hf.LAG,\n",
    "   hf.STARTDATE,\n",
    "   nvl(hf.BASEFCST,0) AS \"BASEFCST\",\n",
    "   nvl(hf.NONBASEFCST,0) AS \"NONBASEFCST\",\n",
    "   nvl(hf.TOTFCST,0) AS \"TOTFCST\",\n",
    "   nvl(hf.RECONCILEDFCST,0) AS \"RECONCILEDFCST\",\n",
    "   nvl(hf.FCSTOVERRIDE,0) AS \"FCSTOVERRIDE\",\n",
    "   nvl(hf.EXTERNALEVENTS,0) AS \"EXTERNALEVENTS\"\n",
    "FROM SCPDBINSTANCE.fcstperfstatichist hf\n",
    "JOIN SCPDBINSTANCE.sku s ON hf.dmdunit = s.item AND hf.loc = s.loc\n",
    "JOIN SCPDBINSTANCE.item i ON hf.dmdunit = i.item AND s.item = i.item\n",
    "WHERE hf.lag in (1, 4)\n",
    "AND s.loc in ('DIV 03', 'DIV 08', 'DIV 14', 'DIV 16', 'DIV 20', 'DIV 21', 'DIV 30', 'DIV 37', 'DIV 41', 'DIV 44', 'DIV 45', 'DIV 48', 'DIV 49', 'DIV 50', 'DIV 51', 'DIV 53', 'DIV 56', 'DIV 58', 'DIV 59', 'DIV 60', 'DIV 64', 'DIV 65', 'DIV 66', 'DIV 67', 'DIV 68', 'DIV 69', 'DIV 70', 'DIV 71', 'DIV 78', 'DIV 80', 'DIV 82', 'DIV 84', 'DIV 85', 'DIV 87', 'DIV 89', 'DIV 90', 'DIV 91', 'DIV 92', 'DIV 93', 'DIV 94', 'DIV 96', 'DIV 98')\n",
    "AND i.item NOT LIKE '8%' -- Excludes customer owned inventory\n",
    "AND hf.startdate between trunc(sysdate,'D') - {oldestsunday} and trunc(sysdate,'D') - {newestsunday}\n",
    "\"\"\"\n",
    "\n",
    "# read the query into a dataframe\n",
    "df_fa_edw = pd.read_sql(query_edw, con=connection)\n",
    "df_fa_by = pd.read_sql(query_by, con=connection)\n",
    "\n",
    "# Close database connection\n",
    "connection.close()\n",
    "\n",
    "print(\"Sales Facts # of records returned: \", len(df_fa_edw))\n",
    "print(\"Blue Yonder # of records returned: \", len(df_fa_by))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of unique STARTDATEs from Blue Yonder DATA: 8\n",
      "   STARTDATE  ORDERED_QTY_PUOM\n",
      "0 2024-09-08           1630146\n",
      "1 2024-09-15           1542804\n",
      "2 2024-09-22           1552258\n",
      "3 2024-09-29           5416777\n",
      "4 2024-10-06           1580497\n",
      "5 2024-10-13           1544337\n",
      "6 2024-10-20           1526002\n",
      "7 2024-10-27           1666588\n",
      "   STARTDATE  ID_SKU\n",
      "0 2024-09-08  158355\n",
      "1 2024-09-15  157516\n",
      "2 2024-09-22  157627\n",
      "3 2024-09-29  156389\n",
      "4 2024-10-06  154958\n",
      "5 2024-10-13  154778\n",
      "6 2024-10-20  155052\n",
      "7 2024-10-27  154710\n",
      "   STARTDATE  ID_SKU\n",
      "0 2024-09-08   47250\n",
      "1 2024-09-15   47443\n",
      "2 2024-09-22   47176\n",
      "3 2024-09-29   46634\n",
      "4 2024-10-06   46324\n",
      "5 2024-10-13   46185\n",
      "6 2024-10-20   46502\n",
      "7 2024-10-27   46228\n"
     ]
    }
   ],
   "source": [
    "# View date ranges for both data pulls\n",
    "print('# of unique STARTDATEs from Blue Yonder DATA:', df_fa_by['STARTDATE'].nunique())\n",
    "\n",
    "# Group by 'STARTDATE' and sum 'ORDERED_QTY_PUOM'\n",
    "sumbySTARTDATE = df_fa_edw.groupby('STARTDATE')['ORDERED_QTY_PUOM'].sum().reset_index()\n",
    "countbySTARTDATE = df_fa_edw.groupby('STARTDATE')['ID_SKU'].count().reset_index()\n",
    "countuniquebySTARTDATE = df_fa_edw.groupby('STARTDATE')['ID_SKU'].nunique().reset_index()\n",
    "\n",
    "\n",
    "print(sumbySTARTDATE)\n",
    "print(countbySTARTDATE)\n",
    "print(countuniquebySTARTDATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in our new DC-item segmentation data\n",
    "seg = pd.read_excel('Inputs/ABCXYZ segmentation.xlsx', dtype={'ID_DIVISION': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format STARTDATEs into datetime\n",
    "df_fa_edw['STARTDATE'] = pd.to_datetime(df_fa_edw['STARTDATE']).dt.date\n",
    "df_fa_by['STARTDATE'] = pd.to_datetime(df_fa_by['STARTDATE']).dt.date\n",
    "\n",
    "### Bring in EDW sales data to BY histfcst data\n",
    "df_fa = pd.merge(df_fa_by, df_fa_edw, on=['ID_DIVISION', 'ID_SKU', 'STARTDATE'], how='left')\n",
    "\n",
    "### Bring in segmentation data\n",
    "df_fa = pd.merge(df_fa, seg, on=['ID_DIVISION', 'ID_SKU'], how='left')\n",
    "\n",
    "# Fill missing values in the 'Segmentation' column with 'Unclassified'\n",
    "df_fa['Segment'].fillna('Unclassified', inplace=True)\n",
    "\n",
    "# Format date column to datetime again\n",
    "df_fa['STARTDATE'] = pd.to_datetime(df_fa['STARTDATE'])\n",
    "\n",
    "# Capture the most recent startdate\n",
    "lastSTARTDATE = df_fa['STARTDATE'].max().strftime('%Y-%m-%d')\n",
    "\n",
    "df_fa1 = df_fa\n",
    "\n",
    "# EDW data for the most recent STARTDATE is empty, remove from analysis\n",
    "#df_fa1 = df_fa[df_fa['STARTDATE'] != mostrecentSTARTDATE]\n",
    "\n",
    "#secondmostrecentSTARTDATE = df_fa1['STARTDATE'].max().strftime('%Y-%m-%d')\n",
    "\n",
    "# Save a copy of raw data\n",
    "#df_fa1.to_csv(f'Outputs/fcstaccuracy data pull 8wk {lastSTARTDATE}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of rows in the final rolled up report:  440633\n"
     ]
    }
   ],
   "source": [
    "### Create calculated columns, then afterwards roll up per Demand Planning's specifications\n",
    "\n",
    "df_fa2 = df_fa1\n",
    "\n",
    "# Replace negative values in 'TOTFCST' column with 0\n",
    "df_fa2['TOTFCST'] = df_fa2['TOTFCST'].clip(lower=0)\n",
    "\n",
    "df_fa2['BASEFCST+RECONCILEDFCST'] = df_fa2['BASEFCST'] + df_fa2['RECONCILEDFCST']\n",
    "df_fa2['TotalATSFcstAdd'] = df_fa2['FCSTOVERRIDE'] + df_fa2['EXTERNALEVENTS']\n",
    "df_fa2['TOTFCST w/o ATS'] = df_fa2['TOTFCST'] - df_fa2['TotalATSFcstAdd']\n",
    "\n",
    "# Create Error delta columns at SKU level\n",
    "df_fa2['BASEFCST Abs Error'] = (df_fa2['BASEFCST'] - df_fa2['ORDERED_QTY_PUOM']).abs()\n",
    "df_fa2['BASEFCST+RECONCILEDFCST Abs Error'] = (df_fa2['BASEFCST+RECONCILEDFCST'] - df_fa2['ORDERED_QTY_PUOM']).abs()\n",
    "df_fa2['TOTFCST Abs Error'] = (df_fa2['TOTFCST'] - df_fa2['ORDERED_QTY_PUOM']).abs()\n",
    "df_fa2['TOTFCST w/o ATS Abs Error'] = (df_fa2['TOTFCST w/o ATS'] - df_fa2['ORDERED_QTY_PUOM']).abs()\n",
    "\n",
    "# Create column ATS Error based on the given IF statements\n",
    "condition1 = (df_fa2['TotalATSFcstAdd'].abs() > 0) & (df_fa2['TOTFCST'] == 0) & (df_fa2['ORDERED_QTY_PUOM'] > 0)\n",
    "df_fa2.loc[condition1, 'ATS Error'] = df_fa2['ORDERED_QTY_PUOM']\n",
    "\n",
    "condition2 = (df_fa2['TotalATSFcstAdd'].abs() > 0)\n",
    "df_fa2.loc[condition2, 'ATS Error'] = df_fa2['TOTFCST Abs Error'] - df_fa2['TOTFCST w/o ATS Abs Error']\n",
    "\n",
    "df_fa2['ATS Error'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Create 'Ordered > TOTFCST (Over)' column\n",
    "df_fa2['Ordered > TOTFCST (Over)'] = df_fa2['ORDERED_QTY_PUOM'].sub(df_fa2['TOTFCST'])\n",
    "df_fa2['Ordered > TOTFCST (Over)'] = df_fa2['Ordered > TOTFCST (Over)'].where(df_fa2['ORDERED_QTY_PUOM'] > df_fa2['TOTFCST'], 0)\n",
    "\n",
    "# Create 'Ordered < TOTFCST (Under)' column\n",
    "df_fa2['Ordered < TOTFCST (Under)'] = df_fa2['TOTFCST'].sub(df_fa2['ORDERED_QTY_PUOM'])\n",
    "df_fa2['Ordered < TOTFCST (Under)'] = df_fa2['Ordered < TOTFCST (Under)'].where(df_fa2['TOTFCST'] > df_fa2['ORDERED_QTY_PUOM'], 0)\n",
    "\n",
    "# Create 'Ordered Absolute Error Delta' column\n",
    "df_fa2['Ordered Absolute Error Delta'] = df_fa2['TOTFCST'].sub(df_fa2['ORDERED_QTY_PUOM']).abs()\n",
    "\n",
    "# Create 'Shipped > TOTFCST (Over)' column\n",
    "df_fa2['Shipped > TOTFCST (Over)'] = df_fa2['SHIPPED_QTY_PUOM'].sub(df_fa2['TOTFCST'])\n",
    "df_fa2['Shipped > TOTFCST (Over)'] = df_fa2['Shipped > TOTFCST (Over)'].where(df_fa2['SHIPPED_QTY_PUOM'] > df_fa2['TOTFCST'], 0)\n",
    "\n",
    "# Create 'Shipped < TOTFCST (Under)' column\n",
    "df_fa2['Shipped < TOTFCST (Under)'] = df_fa2['TOTFCST'].sub(df_fa2['SHIPPED_QTY_PUOM'])\n",
    "df_fa2['Shipped < TOTFCST (Under)'] = df_fa2['Shipped < TOTFCST (Under)'].where(df_fa2['TOTFCST'] > df_fa2['SHIPPED_QTY_PUOM'], 0)\n",
    "\n",
    "# Create 'Shipped Absolute Error Delta' column\n",
    "df_fa2['Shipped Absolute Error Delta'] = df_fa2['TOTFCST'].sub(df_fa2['SHIPPED_QTY_PUOM']).abs()\n",
    "\n",
    "\n",
    "\n",
    "# List of columns to be multiplied\n",
    "columns_to_multiply = ['BASEFCST', 'NONBASEFCST', 'TOTFCST', 'RECONCILEDFCST', 'FCSTOVERRIDE',\n",
    "                       'EXTERNALEVENTS', 'ORDERED_QTY_PUOM', 'SHIPPED_QTY_PUOM', 'BASEFCST+RECONCILEDFCST',\n",
    "                       'TotalATSFcstAdd', 'TOTFCST w/o ATS', 'BASEFCST Abs Error', 'BASEFCST+RECONCILEDFCST Abs Error',\n",
    "                       'TOTFCST Abs Error', 'TOTFCST w/o ATS Abs Error', 'ATS Error']\n",
    "\n",
    "# Multiply each column by 'Unit_Cost_PUOM' and add '$' in front of the column name\n",
    "for column in columns_to_multiply:\n",
    "    df_fa2['$' + column] = df_fa2[column] * df_fa2['Unit_Cost_PUOM']\n",
    "\n",
    "\n",
    "##### Roll up qtys and remove OM SKU ####\n",
    "df_fa2_rollup = df_fa2.groupby(['ID_DIVISION',\n",
    "                                #'ID_SKU',\n",
    "                                'VENDORNUM',\n",
    "                                'Major HPIS Class - Description',\n",
    "                                'Segment',\n",
    "                                'LAG',\n",
    "                                'STARTDATE'\n",
    "                                ])[['BASEFCST','NONBASEFCST','TOTFCST','RECONCILEDFCST','FCSTOVERRIDE','EXTERNALEVENTS','ORDERED_QTY_PUOM','SHIPPED_QTY_PUOM',\n",
    "                                    'BASEFCST+RECONCILEDFCST','TotalATSFcstAdd','TOTFCST w/o ATS',\n",
    "                                    'Ordered > TOTFCST (Over)','Ordered < TOTFCST (Under)','Ordered Absolute Error Delta',\n",
    "                                    'Shipped > TOTFCST (Over)','Shipped < TOTFCST (Under)','Shipped Absolute Error Delta',\n",
    "                                    'BASEFCST Abs Error','BASEFCST+RECONCILEDFCST Abs Error','TOTFCST Abs Error','TOTFCST w/o ATS Abs Error','ATS Error',\n",
    "                                    '$BASEFCST','$NONBASEFCST','$TOTFCST','$RECONCILEDFCST','$FCSTOVERRIDE','$EXTERNALEVENTS','$ORDERED_QTY_PUOM','$SHIPPED_QTY_PUOM',\n",
    "                                    '$BASEFCST+RECONCILEDFCST','$TotalATSFcstAdd','$TOTFCST w/o ATS',\n",
    "                                    '$BASEFCST Abs Error','$BASEFCST+RECONCILEDFCST Abs Error','$TOTFCST Abs Error','$TOTFCST w/o ATS Abs Error','$ATS Error'\n",
    "                                    ]].sum().reset_index()\n",
    "\n",
    "# Separate out lags\n",
    "df_fa2_rollup_lag1 = df_fa2_rollup[df_fa2_rollup['LAG'] == 1]\n",
    "df_fa2_rollup_lag4 = df_fa2_rollup[df_fa2_rollup['LAG'] == 4]\n",
    "\n",
    "print(\"# of rows in the final rolled up report: \", len(df_fa2_rollup_lag1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of rows in DC rollup:  320\n",
      "# of rows in Network rollup:  8\n"
     ]
    }
   ],
   "source": [
    "# Filter for only lag 1 for DC and network rollups\n",
    "df_fa2_lag1 = df_fa2[df_fa2['LAG'] == 1]\n",
    "\n",
    "# Rollup to DC by STARTDATE\n",
    "df_fa2_DCrollup_lag1 = df_fa2_lag1.groupby(['ID_DIVISION',\n",
    "                                #'ID_SKU',\n",
    "                                #'VENDORNUM',\n",
    "                                #'Major HPIS Class - Description',\n",
    "                                #'Segment',\n",
    "                                #'LAG',\n",
    "                                'STARTDATE'\n",
    "                                ])[['BASEFCST','NONBASEFCST','TOTFCST','RECONCILEDFCST','FCSTOVERRIDE','EXTERNALEVENTS','ORDERED_QTY_PUOM','SHIPPED_QTY_PUOM',\n",
    "                                    'BASEFCST+RECONCILEDFCST','TotalATSFcstAdd','TOTFCST w/o ATS',\n",
    "                                    'Ordered > TOTFCST (Over)','Ordered < TOTFCST (Under)','Ordered Absolute Error Delta',\n",
    "                                    'Shipped > TOTFCST (Over)','Shipped < TOTFCST (Under)','Shipped Absolute Error Delta',\n",
    "                                    'BASEFCST Abs Error','BASEFCST+RECONCILEDFCST Abs Error','TOTFCST Abs Error','TOTFCST w/o ATS Abs Error','ATS Error',\n",
    "                                    '$BASEFCST','$NONBASEFCST','$TOTFCST','$RECONCILEDFCST','$FCSTOVERRIDE','$EXTERNALEVENTS','$ORDERED_QTY_PUOM','$SHIPPED_QTY_PUOM',\n",
    "                                    '$BASEFCST+RECONCILEDFCST','$TotalATSFcstAdd','$TOTFCST w/o ATS',\n",
    "                                    '$BASEFCST Abs Error','$BASEFCST+RECONCILEDFCST Abs Error','$TOTFCST Abs Error','$TOTFCST w/o ATS Abs Error','$ATS Error'\n",
    "                                    ]].sum().reset_index()\n",
    "\n",
    "\n",
    "# Rollup to network by STARTDATE\n",
    "df_fa2_Networkrollup_lag1 = df_fa2_lag1.groupby([#'ID_DIVISION',\n",
    "                                #'ID_SKU',\n",
    "                                #'VENDORNUM',\n",
    "                                #'Major HPIS Class - Description',\n",
    "                                #'Segment',\n",
    "                                #'LAG',\n",
    "                                'STARTDATE'\n",
    "                                ])[['BASEFCST','NONBASEFCST','TOTFCST','RECONCILEDFCST','FCSTOVERRIDE','EXTERNALEVENTS','ORDERED_QTY_PUOM','SHIPPED_QTY_PUOM',\n",
    "                                    'BASEFCST+RECONCILEDFCST','TotalATSFcstAdd','TOTFCST w/o ATS',\n",
    "                                    'Ordered > TOTFCST (Over)','Ordered < TOTFCST (Under)','Ordered Absolute Error Delta',\n",
    "                                    'Shipped > TOTFCST (Over)','Shipped < TOTFCST (Under)','Shipped Absolute Error Delta',\n",
    "                                    'BASEFCST Abs Error','BASEFCST+RECONCILEDFCST Abs Error','TOTFCST Abs Error','TOTFCST w/o ATS Abs Error','ATS Error',\n",
    "                                    '$BASEFCST','$NONBASEFCST','$TOTFCST','$RECONCILEDFCST','$FCSTOVERRIDE','$EXTERNALEVENTS','$ORDERED_QTY_PUOM','$SHIPPED_QTY_PUOM',\n",
    "                                    '$BASEFCST+RECONCILEDFCST','$TotalATSFcstAdd','$TOTFCST w/o ATS',\n",
    "                                    '$BASEFCST Abs Error','$BASEFCST+RECONCILEDFCST Abs Error','$TOTFCST Abs Error','$TOTFCST w/o ATS Abs Error','$ATS Error'\n",
    "                                    ]].sum().reset_index()\n",
    "                 \n",
    "print(\"# of rows in DC rollup: \", len(df_fa2_DCrollup_lag1))               \n",
    "print(\"# of rows in Network rollup: \", len(df_fa2_Networkrollup_lag1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create dataframe that creates a tab to record ATS Error Logic\n",
    "\n",
    "ATSErrorlogic = {\n",
    "    'Condition': ['Condition 1', 'Condition 2', 'Condition 3'],\n",
    "    'Logic': [\n",
    "        \"IF ABS('TotalATSFcstAdd') > 0 AND 'TOTFCST' >= 0 AND 'ORDERED_QTY_PUOM' == 0, THEN 'ATS Error' = 'ORDERED_QTY_PUOM'\",\n",
    "        \"IF ABS('TotalATSFcstAdd') > 0, THEN 'ATS Error' = 'TOTFCST Abs Error' - 'TOTFCST w/o ATS Abs Error'\",\n",
    "        \"ELSE 'ATS Error' = 0\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "ATSErrorlogic = pd.DataFrame(ATSErrorlogic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create 2 dataframes: DC-SKU-LAG-STARTDATE records with large discrepancies between Ordered and shipped qtys, and records for top 10 SKUs with highest ordered error delta for each DC\n",
    "\n",
    "# Filter for only lag 1\n",
    "df_fa2_lag1 = df_fa2[df_fa2['LAG'] == 1]\n",
    "\n",
    "# Create df of DC-SKU-LAG-STARTDATE records where there is a large delta between Ordered vs Shipped\n",
    "df_fa2_lag1_discrep = df_fa2_lag1[(df_fa2_lag1['ORDERED_QTY_PUOM'] >= df_fa2_lag1['SHIPPED_QTY_PUOM'] * 4) & (df_fa2_lag1['SHIPPED_QTY_PUOM'] > 0)]\n",
    "\n",
    "# Group by 'ID_DIVISION' and 'ID_SKU', and sum 'Ordered Absolute Error Delta'\n",
    "df_fa2_lag1_grouped = df_fa2_lag1.groupby(['ID_DIVISION', 'ID_SKU'])['Ordered Absolute Error Delta'].sum().reset_index()\n",
    "\n",
    "# Sort within each 'ID_DIVISION' to grab top 10 'ID_SKU's with the highest sum of 'Ordered Absolute Error Delta'\n",
    "df_fa2_lag1_top10 = df_fa2_lag1_grouped.groupby('ID_DIVISION').apply(lambda x: x.nlargest(10, 'Ordered Absolute Error Delta')).reset_index(drop=True)\n",
    "\n",
    "df_fa2_lag1_top10 = pd.merge(df_fa2_lag1, df_fa2_lag1_top10[['ID_DIVISION', 'ID_SKU']], on=['ID_DIVISION', 'ID_SKU'], how='inner')\n",
    "\n",
    "df_fa2_lag1_top10 = df_fa2_lag1_top10.groupby(['ID_DIVISION',\n",
    "                                'ID_SKU',\n",
    "                                'VENDORNUM',\n",
    "                                'Major HPIS Class - Description',\n",
    "                                'Segment',\n",
    "                                'LAG',\n",
    "                                'STARTDATE'\n",
    "                                ])[['BASEFCST','NONBASEFCST','TOTFCST','RECONCILEDFCST','FCSTOVERRIDE','EXTERNALEVENTS','ORDERED_QTY_PUOM','SHIPPED_QTY_PUOM',\n",
    "                                    'BASEFCST+RECONCILEDFCST','TotalATSFcstAdd','TOTFCST w/o ATS',\n",
    "                                    'Ordered > TOTFCST (Over)','Ordered < TOTFCST (Under)','Ordered Absolute Error Delta',\n",
    "                                    'Shipped > TOTFCST (Over)','Shipped < TOTFCST (Under)','Shipped Absolute Error Delta',\n",
    "                                    'BASEFCST Abs Error','BASEFCST+RECONCILEDFCST Abs Error','TOTFCST Abs Error','TOTFCST w/o ATS Abs Error','ATS Error',\n",
    "                                    '$BASEFCST','$NONBASEFCST','$TOTFCST','$RECONCILEDFCST','$FCSTOVERRIDE','$EXTERNALEVENTS','$ORDERED_QTY_PUOM','$SHIPPED_QTY_PUOM',\n",
    "                                    '$BASEFCST+RECONCILEDFCST','$TotalATSFcstAdd','$TOTFCST w/o ATS',\n",
    "                                    '$BASEFCST Abs Error','$BASEFCST+RECONCILEDFCST Abs Error','$TOTFCST Abs Error','$TOTFCST w/o ATS Abs Error','$ATS Error'\n",
    "                                    ]].sum().reset_index()\n",
    "# Order the dataframe\n",
    "df_fa2_lag1_top10 = df_fa2_lag1_top10.sort_values(by=['ID_DIVISION', 'Ordered Absolute Error Delta'], ascending=[True, False])\n",
    "\n",
    "\n",
    "\n",
    "### Capture the top 100 ID_SKU - STARTDATE combinations at each given DC\n",
    "\n",
    "# Sort the DataFrame by 'ID_DIVISION' and 'ATS Error' in descending order\n",
    "df_fa2_lag1_sorted = df_fa2_lag1.sort_values(by=['ID_DIVISION', 'ATS Error'], ascending=[True, False])\n",
    "\n",
    "# Group by 'ID_DIVISION' and get the top 100 'ID_SKU' and 'STARTDATE' combinations for each group\n",
    "df_fa2_lag1_top100ATSerror = df_fa2_lag1_sorted.groupby('ID_DIVISION').head(100)\n",
    "\n",
    "# Select only the required columns\n",
    "df_fa2_lag1_top100ATSerror = df_fa2_lag1_top100ATSerror[['ID_DIVISION', 'ID_SKU', 'STARTDATE', 'ATS Error', '$ATS Error']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Excel writer instance\n",
    "Filewriter = pd.ExcelWriter(f'Outputs/Forecast Accuracy 8wks thru {lastSTARTDATE}.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# List of dataframes and their respective sheet names\n",
    "dataframes = [\n",
    "    (df_fa2_Networkrollup_lag1, 'Network rollup'),\n",
    "    (df_fa2_DCrollup_lag1, 'DC rollup'),\n",
    "    (df_fa2_rollup_lag1, 'lag1 by Vendor-Segment'),\n",
    "    (df_fa2_rollup_lag4, 'lag4 by Vendor-Segment'),\n",
    "    (df_fa2_lag1_top100ATSerror, 'Top 100 DC-Item ATS Error'),\n",
    "    (df_fa2_lag1_top10, 'Top 10 DC-Item Order deltas'),\n",
    "    (df_fa2_lag1_discrep, 'Large Order v Ship Discrep'),\n",
    "    (ATSErrorlogic, 'ATS Error Logic')\n",
    "]\n",
    "\n",
    "# Write all dataframes to their respective sheets in a loop\n",
    "for df, sheet_name in dataframes:\n",
    "    df.to_excel(Filewriter, sheet_name=sheet_name, index=False)\n",
    "\n",
    "# Get the xlsxwriter workbook and worksheet objects\n",
    "workbook = Filewriter.book\n",
    "\n",
    "# Define formats\n",
    "header_format = workbook.add_format({\n",
    "    'bold': True,\n",
    "    'text_wrap': True,\n",
    "    'valign': 'center',\n",
    "    'align': 'center'\n",
    "})\n",
    "currency_format = workbook.add_format({'num_format': '$#,##0.00'})\n",
    "\n",
    "# Function to format each worksheet\n",
    "def format_worksheet(writer, sheet_name, df):\n",
    "    worksheet = writer.sheets[sheet_name]\n",
    "    for col_num, value in enumerate(df.columns):\n",
    "        worksheet.write(0, col_num, value, header_format)\n",
    "        worksheet.set_column(col_num, col_num, len(value) + 2)  # Set column width based on header length\n",
    "        # Apply currency format to columns where header starts with '$'\n",
    "        if value.startswith('$'):\n",
    "            worksheet.set_column(col_num, col_num, None, currency_format)\n",
    "    \n",
    "    worksheet.autofilter(0, 0, 0, len(df.columns) - 1)  # Add dropdown filters\n",
    "    worksheet.freeze_panes(1, 0)  # Freeze the top row\n",
    "\n",
    "# Apply formatting to all worksheets in a loop\n",
    "for df, sheet_name in dataframes:\n",
    "    format_worksheet(Filewriter, sheet_name, df)\n",
    "\n",
    "# Close the writer\n",
    "Filewriter.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
