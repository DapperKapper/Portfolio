{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import oracledb\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress all Python warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set pandas option to display numbers in plain format\n",
    "pd.set_option('display.float_format', '{:.0f}'.format)\n",
    "\n",
    "# Get today's date\n",
    "today = datetime.today()\n",
    "\n",
    "# Calculate the number of days between today and the most recent Sunday\n",
    "newestsunday = (today.weekday() - 6) % 7\n",
    "\n",
    "# Calculate the number of days between today and X Sundays ago\n",
    "oldestsunday = newestsunday + (7 * 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales Facts # of records returned:  1249385\n",
      "Blue Yonder # of records returned:  6038735\n"
     ]
    }
   ],
   "source": [
    "# Establish the database connection\n",
    "connection = oracledb.connect(user='usernamehere',\n",
    "                              password='pswdhere',\n",
    "                              dsn=oracledb.makedsn('hostnamehere', '1234', service_name='DATABASE_NAME_HERE'))\n",
    "cursor = connection.cursor()\n",
    "\n",
    "query_edw = f\"\"\"\n",
    "SELECT od.ID_DIVISION\n",
    "\t,pd.ID_SKU\n",
    "\t,pgd.CD_HPIS_MAJOR || ' - ' || pgd.DS_HPIS_MAJOR AS \"Major HPIS Class - Description\"\n",
    "\t,(pd.AT_CORP_BASE_COST * pd.RT_PURCHASE_UOM_CONVERSION) AS \"Unit_Cost_PUOM\"\n",
    "    ,SUM(sf.QT_ORDER / pd.RT_PURCHASE_UOM_CONVERSION) AS ORDERED_QTY_PUOM\n",
    "    ,SUM(sf.QT_SHIP / pd.RT_PURCHASE_UOM_CONVERSION) AS SHIPPED_QTY_PUOM\n",
    "\t,CASE \n",
    "\t\tWHEN TO_CHAR(td.KY_TIME, 'D') = 1\n",
    "\t\t\tTHEN TRUNC(td.KY_TIME)\n",
    "\t\tELSE TRUNC(td.KY_TIME, 'IW') - 1\n",
    "\t\tEND AS STARTDATE\n",
    "FROM DBINSTANCE.SALES_FACTS sf\n",
    "JOIN DBINSTANCE.PRODUCT_DIM pd ON pd.KY_PRODUCT = sf.KY_PRODUCT\n",
    "JOIN DBINSTANCE.ORG_DIM od ON sf.KY_ORG = od.KY_ORG\n",
    "JOIN DBINSTANCE.TIME_DIM td ON sf.KY_TIME = td.KY_TIME\n",
    "JOIN DBINSTANCE.INVOICE_TYPE_DIM itd ON itd.KY_INVOICE_TYPE = sf.KY_INVOICE_TYPE\n",
    "LEFT JOIN DBINSTANCE.PRODUCT_GROUP_DIM pgd ON pd.CD_HPIS = pgd.CD_HPIS_CLASS\n",
    "WHERE sf.QT_ORDER > 0\n",
    "\tAND td.KY_TIME BETWEEN TRUNC(SYSDATE) - {oldestsunday}\n",
    "\t\tAND TRUNC(SYSDATE) - {newestsunday} \t\t-- Pulls 60 days worth of data\n",
    "\tAND pd.ID_SKU NOT LIKE '8%' \t\t\t-- Excludes customer owned inventory\n",
    "\tAND itd.CD_INVOICE_TYPE <> 'REBILL'\n",
    "\tAND itd.CD_INVOICE_TYPE <> 'DIRECT' \t-- Filters out direct orders (supplier directly to customer)\n",
    "\tAND itd.FG_ORIGINAL_ORDER = 'ORIGINAL' \t-- Filters out backorders\n",
    "\tAND pd.FG_CURRENT = 'Y'\n",
    "\tAND pd.FG_ACTIVE_SKU = 'ACTIVE'\n",
    "    AND od.ID_DIVISION IN (03, 08, 14, 16, 20, 21, 30, 37, 41, 44, 45, 48, 49, 50, 51, 53, 56, 58, 59, 60, 64, 65, 66, 67, 68, 69, 70, 71, 78, 80, 82, 84, 85, 87, 89, 90, 91, 92, 93, 94, 96, 98)\n",
    "    GROUP BY od.ID_DIVISION, ID_SKU, pgd.CD_HPIS_MAJOR || ' - ' || pgd.DS_HPIS_MAJOR, (pd.AT_CORP_BASE_COST * pd.RT_PURCHASE_UOM_CONVERSION),\n",
    "         CASE\n",
    "             WHEN TO_CHAR(td.KY_TIME, 'D') = 1 THEN TRUNC(td.KY_TIME)\n",
    "             ELSE TRUNC(td.KY_TIME, 'IW') - 1\n",
    "         END\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "query_by = f\"\"\"\n",
    "SELECT SUBSTR(s.loc, LENGTH(s.loc) - 1, 2) AS \"ID_DIVISION\",\n",
    "   i.VENDORNUM,\n",
    "   hf.DMDUNIT AS \"ID_SKU\",\n",
    "   hf.LAG,\n",
    "   hf.STARTDATE,\n",
    "   nvl(hf.BASEFCST,0) AS \"BASEFCST\",\n",
    "   nvl(hf.NONBASEFCST,0) AS \"NONBASEFCST\",\n",
    "   nvl(hf.TOTFCST,0) AS \"TOTFCST\",\n",
    "   nvl(hf.RECONCILEDFCST,0) AS \"RECONCILEDFCST\",\n",
    "   nvl(hf.FCSTOVERRIDE,0) AS \"FCSTOVERRIDE\",\n",
    "   nvl(hf.EXTERNALEVENTS,0) AS \"EXTERNALEVENTS\"\n",
    "FROM SCPDBINSTANCE.fcstperfstatichist hf\n",
    "JOIN SCPDBINSTANCE.sku s ON hf.dmdunit = s.item AND hf.loc = s.loc\n",
    "JOIN SCPDBINSTANCE.item i ON hf.dmdunit = i.item AND s.item = i.item\n",
    "WHERE hf.lag in (1, 4)\n",
    "AND s.loc in ('DIV 03', 'DIV 08', 'DIV 14', 'DIV 16', 'DIV 20', 'DIV 21', 'DIV 30', 'DIV 37', 'DIV 41', 'DIV 44', 'DIV 45', 'DIV 48', 'DIV 49', 'DIV 50', 'DIV 51', 'DIV 53', 'DIV 56', 'DIV 58', 'DIV 59', 'DIV 60', 'DIV 64', 'DIV 65', 'DIV 66', 'DIV 67', 'DIV 68', 'DIV 69', 'DIV 70', 'DIV 71', 'DIV 78', 'DIV 80', 'DIV 82', 'DIV 84', 'DIV 85', 'DIV 87', 'DIV 89', 'DIV 90', 'DIV 91', 'DIV 92', 'DIV 93', 'DIV 94', 'DIV 96', 'DIV 98')\n",
    "AND i.item NOT LIKE '8%' -- Excludes customer owned inventory\n",
    "AND hf.startdate between trunc(sysdate,'D') - {oldestsunday} and trunc(sysdate,'D') - {newestsunday}\n",
    "\"\"\"\n",
    "\n",
    "# read the query into a dataframe\n",
    "df_fa_edw = pd.read_sql(query_edw, con=connection)\n",
    "df_fa_by = pd.read_sql(query_by, con=connection)\n",
    "\n",
    "# Close database connection\n",
    "connection.close()\n",
    "\n",
    "print(\"Sales Facts # of records returned: \", len(df_fa_edw))\n",
    "print(\"Blue Yonder # of records returned: \", len(df_fa_by))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of unique STARTDATEs from Blue Yonder DATA: 8\n",
      "   STARTDATE  ORDERED_QTY_PUOM\n",
      "0 2024-09-08           1630146\n",
      "1 2024-09-15           1542804\n",
      "2 2024-09-22           1552258\n",
      "3 2024-09-29           5416777\n",
      "4 2024-10-06           1580497\n",
      "5 2024-10-13           1544337\n",
      "6 2024-10-20           1526002\n",
      "7 2024-10-27           1666588\n",
      "   STARTDATE  ID_SKU\n",
      "0 2024-09-08  158355\n",
      "1 2024-09-15  157516\n",
      "2 2024-09-22  157627\n",
      "3 2024-09-29  156389\n",
      "4 2024-10-06  154958\n",
      "5 2024-10-13  154778\n",
      "6 2024-10-20  155052\n",
      "7 2024-10-27  154710\n",
      "   STARTDATE  ID_SKU\n",
      "0 2024-09-08   47250\n",
      "1 2024-09-15   47443\n",
      "2 2024-09-22   47176\n",
      "3 2024-09-29   46634\n",
      "4 2024-10-06   46324\n",
      "5 2024-10-13   46185\n",
      "6 2024-10-20   46502\n",
      "7 2024-10-27   46228\n"
     ]
    }
   ],
   "source": [
    "# View date ranges for both data pulls\n",
    "print('# of unique STARTDATEs from Blue Yonder DATA:', df_fa_by['STARTDATE'].nunique())\n",
    "\n",
    "# Group by 'STARTDATE' and sum 'ORDERED_QTY_PUOM'\n",
    "sumbySTARTDATE = df_fa_edw.groupby('STARTDATE')['ORDERED_QTY_PUOM'].sum().reset_index()\n",
    "countbySTARTDATE = df_fa_edw.groupby('STARTDATE')['ID_SKU'].count().reset_index()\n",
    "countuniquebySTARTDATE = df_fa_edw.groupby('STARTDATE')['ID_SKU'].nunique().reset_index()\n",
    "\n",
    "\n",
    "print(sumbySTARTDATE)\n",
    "print(countbySTARTDATE)\n",
    "print(countuniquebySTARTDATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in our new DC-item segmentation data\n",
    "seg = pd.read_excel('Inputs/ABCXYZ segmentation.xlsx', dtype={'ID_DIVISION': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format STARTDATEs into datetime\n",
    "df_fa_edw['STARTDATE'] = pd.to_datetime(df_fa_edw['STARTDATE']).dt.date\n",
    "df_fa_by['STARTDATE'] = pd.to_datetime(df_fa_by['STARTDATE']).dt.date\n",
    "\n",
    "### Bring in EDW sales data to BY histfcst data\n",
    "df_fa = pd.merge(df_fa_by, df_fa_edw, on=['ID_DIVISION', 'ID_SKU', 'STARTDATE'], how='left')\n",
    "\n",
    "### Bring in segmentation data\n",
    "df_fa = pd.merge(df_fa, seg, on=['ID_DIVISION', 'ID_SKU'], how='left')\n",
    "\n",
    "# Fill missing values in the 'Segmentation' column with 'Unclassified'\n",
    "df_fa['Segment'].fillna('Unclassified', inplace=True)\n",
    "\n",
    "# Format date column to datetime again\n",
    "df_fa['STARTDATE'] = pd.to_datetime(df_fa['STARTDATE'])\n",
    "\n",
    "# Capture the most recent startdate\n",
    "lastSTARTDATE = df_fa['STARTDATE'].max().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of rows in the final rolled up report:  440633\n"
     ]
    }
   ],
   "source": [
    "# Start with a copy of the DataFrame\n",
    "df_fa2 = df_fa.copy()\n",
    "\n",
    "# Clip negative values in 'TOTFCST'\n",
    "df_fa2['TOTFCST'] = df_fa2['TOTFCST'].clip(lower=0)\n",
    "\n",
    "# Helper function to create error columns\n",
    "def calculate_abs_error(forecast_column, actual_column):\n",
    "    return (df_fa2[forecast_column] - df_fa2[actual_column]).abs()\n",
    "\n",
    "# Create calculated columns\n",
    "df_fa2 = df_fa2.assign(\n",
    "    **{\n",
    "        'BASEFCST+RECONCILEDFCST': df_fa2['BASEFCST'] + df_fa2['RECONCILEDFCST'],\n",
    "        'TotalATSFcstAdd': df_fa2['FCSTOVERRIDE'] + df_fa2['EXTERNALEVENTS'],\n",
    "        'TOTFCST w/o ATS': df_fa2['TOTFCST'] - (df_fa2['FCSTOVERRIDE'] + df_fa2['EXTERNALEVENTS']),\n",
    "        'BASEFCST Abs Error': calculate_abs_error('BASEFCST', 'ORDERED_QTY_PUOM'),\n",
    "        'BASEFCST+RECONCILEDFCST Abs Error': calculate_abs_error('BASEFCST+RECONCILEDFCST', 'ORDERED_QTY_PUOM'),\n",
    "        'TOTFCST Abs Error': calculate_abs_error('TOTFCST', 'ORDERED_QTY_PUOM'),\n",
    "        'TOTFCST w/o ATS Abs Error': calculate_abs_error('TOTFCST w/o ATS', 'ORDERED_QTY_PUOM'),\n",
    "        'Ordered > TOTFCST (Over)': df_fa2['ORDERED_QTY_PUOM'].sub(df_fa2['TOTFCST']).clip(lower=0),\n",
    "        'Ordered < TOTFCST (Under)': df_fa2['TOTFCST'].sub(df_fa2['ORDERED_QTY_PUOM']).clip(lower=0),\n",
    "        'Ordered Absolute Error Delta': (df_fa2['TOTFCST'] - df_fa2['ORDERED_QTY_PUOM']).abs(),\n",
    "        'Shipped > TOTFCST (Over)': df_fa2['SHIPPED_QTY_PUOM'].sub(df_fa2['TOTFCST']).clip(lower=0),\n",
    "        'Shipped < TOTFCST (Under)': df_fa2['TOTFCST'].sub(df_fa2['SHIPPED_QTY_PUOM']).clip(lower=0),\n",
    "        'Shipped Absolute Error Delta': (df_fa2['TOTFCST'] - df_fa2['SHIPPED_QTY_PUOM']).abs()\n",
    "    }\n",
    ")\n",
    "\n",
    "# Set 'ATS Error' based on conditions\n",
    "df_fa2['ATS Error'] = 0\n",
    "df_fa2.loc[(df_fa2['TotalATSFcstAdd'].abs() > 0) & (df_fa2['TOTFCST'] == 0) & (df_fa2['ORDERED_QTY_PUOM'] > 0), 'ATS Error'] = df_fa2['ORDERED_QTY_PUOM']\n",
    "df_fa2.loc[df_fa2['TotalATSFcstAdd'].abs() > 0, 'ATS Error'] = df_fa2['TOTFCST Abs Error'] - df_fa2['TOTFCST w/o ATS Abs Error']\n",
    "\n",
    "# Create currency columns\n",
    "columns_to_multiply = [\n",
    "    'BASEFCST', 'NONBASEFCST', 'TOTFCST', 'RECONCILEDFCST', 'FCSTOVERRIDE',\n",
    "    'EXTERNALEVENTS', 'ORDERED_QTY_PUOM', 'SHIPPED_QTY_PUOM', 'BASEFCST+RECONCILEDFCST',\n",
    "    'TotalATSFcstAdd', 'TOTFCST w/o ATS', 'BASEFCST Abs Error', 'BASEFCST+RECONCILEDFCST Abs Error',\n",
    "    'TOTFCST Abs Error', 'TOTFCST w/o ATS Abs Error', 'ATS Error'\n",
    "]\n",
    "df_fa2 = df_fa2.assign(**{f'${col}': df_fa2[col] * df_fa2['Unit_Cost_PUOM'] for col in columns_to_multiply})\n",
    "\n",
    "# Roll up per specifications\n",
    "df_fa2_rollup = (\n",
    "    df_fa2.groupby(\n",
    "        ['ID_DIVISION', 'VENDORNUM', 'Major HPIS Class - Description', 'Segment', 'LAG', 'STARTDATE'],\n",
    "        as_index=False\n",
    "    )\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "# Separate out LAGs\n",
    "df_fa2_rollup_lag1 = df_fa2_rollup[df_fa2_rollup['LAG'] == 1]\n",
    "df_fa2_rollup_lag4 = df_fa2_rollup[df_fa2_rollup['LAG'] == 4]\n",
    "\n",
    "print(\"# of rows in the final rolled-up report: \", len(df_fa2_rollup_lag1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of rows in DC rollup:  320\n",
      "# of rows in Network rollup:  8\n"
     ]
    }
   ],
   "source": [
    "# Define list of columns to aggregate to avoid repetition\n",
    "columns_to_aggregate = [\n",
    "    'BASEFCST', 'NONBASEFCST', 'TOTFCST', 'RECONCILEDFCST', 'FCSTOVERRIDE', 'EXTERNALEVENTS', \n",
    "    'ORDERED_QTY_PUOM', 'SHIPPED_QTY_PUOM', 'BASEFCST+RECONCILEDFCST', 'TotalATSFcstAdd', \n",
    "    'TOTFCST w/o ATS', 'Ordered > TOTFCST (Over)', 'Ordered < TOTFCST (Under)', \n",
    "    'Ordered Absolute Error Delta', 'Shipped > TOTFCST (Over)', 'Shipped < TOTFCST (Under)', \n",
    "    'Shipped Absolute Error Delta', 'BASEFCST Abs Error', 'BASEFCST+RECONCILEDFCST Abs Error', \n",
    "    'TOTFCST Abs Error', 'TOTFCST w/o ATS Abs Error', 'ATS Error', '$BASEFCST', '$NONBASEFCST', \n",
    "    '$TOTFCST', '$RECONCILEDFCST', '$FCSTOVERRIDE', '$EXTERNALEVENTS', '$ORDERED_QTY_PUOM', \n",
    "    '$SHIPPED_QTY_PUOM', '$BASEFCST+RECONCILEDFCST', '$TotalATSFcstAdd', '$TOTFCST w/o ATS', \n",
    "    '$BASEFCST Abs Error', '$BASEFCST+RECONCILEDFCST Abs Error', '$TOTFCST Abs Error', \n",
    "    '$TOTFCST w/o ATS Abs Error', '$ATS Error'\n",
    "]\n",
    "\n",
    "# Filter data for LAG = 1\n",
    "df_fa2_lag1 = df_fa2[df_fa2['LAG'] == 1]\n",
    "\n",
    "# Function to perform rollup based on specified groupby columns\n",
    "def perform_rollup(dataframe, groupby_columns, aggregate_columns):\n",
    "    return dataframe.groupby(groupby_columns)[aggregate_columns].sum().reset_index()\n",
    "\n",
    "# Rollup to DC by STARTDATE\n",
    "df_fa2_DCrollup_lag1 = perform_rollup(\n",
    "    dataframe=df_fa2_lag1, \n",
    "    groupby_columns=['ID_DIVISION', 'STARTDATE'], \n",
    "    aggregate_columns=columns_to_aggregate\n",
    ")\n",
    "\n",
    "# Rollup to Network by STARTDATE\n",
    "df_fa2_Networkrollup_lag1 = perform_rollup(\n",
    "    dataframe=df_fa2_lag1, \n",
    "    groupby_columns=['STARTDATE'], \n",
    "    aggregate_columns=columns_to_aggregate\n",
    ")\n",
    "\n",
    "print(\"# of rows in DC rollup:\", len(df_fa2_DCrollup_lag1))\n",
    "print(\"# of rows in Network rollup:\", len(df_fa2_Networkrollup_lag1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create dataframes that identify large discrepancies when ATS qtys come into play\n",
    "\n",
    "# Filter for only lag 1 records\n",
    "df_fa2_lag1 = df_fa2[df_fa2['LAG'] == 1]\n",
    "\n",
    "# Helper function to get records with large discrepancies between Ordered and Shipped quantities\n",
    "def filter_large_discrepancy(dataframe):\n",
    "    return dataframe[(dataframe['ORDERED_QTY_PUOM'] >= dataframe['SHIPPED_QTY_PUOM'] * 4) & (dataframe['SHIPPED_QTY_PUOM'] > 0)]\n",
    "\n",
    "# Helper function to get top SKUs by Ordered Absolute Error Delta for each DC\n",
    "def get_top_skus_by_error(dataframe, top_n=10):\n",
    "    grouped = dataframe.groupby(['ID_DIVISION', 'ID_SKU'])['Ordered Absolute Error Delta'].sum().reset_index()\n",
    "    top_skus = grouped.groupby('ID_DIVISION').apply(lambda x: x.nlargest(top_n, 'Ordered Absolute Error Delta')).reset_index(drop=True)\n",
    "    # Merge back to get complete records for these top SKUs\n",
    "    return pd.merge(dataframe, top_skus[['ID_DIVISION', 'ID_SKU']], on=['ID_DIVISION', 'ID_SKU'], how='inner')\n",
    "\n",
    "# Helper function to roll up aggregated data\n",
    "def perform_rollup(dataframe, groupby_columns):\n",
    "    return dataframe.groupby(groupby_columns)[columns_to_aggregate].sum().reset_index()\n",
    "\n",
    "# Large discrepancy records for DC-SKU-LAG-STARTDATE\n",
    "df_fa2_lag1_discrep = filter_large_discrepancy(df_fa2_lag1)\n",
    "\n",
    "# Top 10 SKUs with the highest ordered error delta for each DC\n",
    "df_fa2_lag1_top10 = get_top_skus_by_error(df_fa2_lag1, top_n=10)\n",
    "df_fa2_lag1_top10 = perform_rollup(df_fa2_lag1_top10, ['ID_DIVISION', 'ID_SKU', 'VENDORNUM', 'Major HPIS Class - Description', \n",
    "                                                      'Segment', 'LAG', 'STARTDATE'])\n",
    "# Sort the result\n",
    "df_fa2_lag1_top10 = df_fa2_lag1_top10.sort_values(by=['ID_DIVISION', 'Ordered Absolute Error Delta'], ascending=[True, False])\n",
    "\n",
    "# Capture top 100 ID_SKU - STARTDATE combinations by ATS Error at each DC\n",
    "df_fa2_lag1_sorted = df_fa2_lag1.sort_values(by=['ID_DIVISION', 'ATS Error'], ascending=[True, False])\n",
    "df_fa2_lag1_top100ATSerror = df_fa2_lag1_sorted.groupby('ID_DIVISION').head(100)\n",
    "\n",
    "# Select the required columns for the top 100 records\n",
    "df_fa2_lag1_top100ATSerror = df_fa2_lag1_top100ATSerror[['ID_DIVISION', 'ID_SKU', 'STARTDATE', 'ATS Error', '$ATS Error']]\n",
    "\n",
    "# Display row counts\n",
    "print(\"# of rows in large discrepancy records:\", len(df_fa2_lag1_discrep))\n",
    "print(\"# of rows in top 10 SKUs by error:\", len(df_fa2_lag1_top10))\n",
    "print(\"# of rows in top 100 ATS Error records per DC:\", len(df_fa2_lag1_top100ATSerror))\n",
    "\n",
    "\n",
    "\n",
    "### Create dataframe used to define ATS Error Logic\n",
    "\n",
    "ATSErrorlogic = {\n",
    "    'Condition': ['Condition 1', 'Condition 2', 'Condition 3'],\n",
    "    'Logic': [\n",
    "        \"IF ABS('TotalATSFcstAdd') > 0 AND 'TOTFCST' >= 0 AND 'ORDERED_QTY_PUOM' == 0, THEN 'ATS Error' = 'ORDERED_QTY_PUOM'\",\n",
    "        \"IF ABS('TotalATSFcstAdd') > 0, THEN 'ATS Error' = 'TOTFCST Abs Error' - 'TOTFCST w/o ATS Abs Error'\",\n",
    "        \"ELSE 'ATS Error' = 0\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "ATSErrorlogic = pd.DataFrame(ATSErrorlogic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Excel writer instance\n",
    "Filewriter = pd.ExcelWriter(f'Outputs/Forecast Accuracy 8wks thru {lastSTARTDATE}.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# List of dataframes and their respective sheet names\n",
    "dataframes = [\n",
    "    (df_fa2_Networkrollup_lag1, 'Network rollup'),\n",
    "    (df_fa2_DCrollup_lag1, 'DC rollup'),\n",
    "    (df_fa2_rollup_lag1, 'lag1 by Vendor-Segment'),\n",
    "    (df_fa2_rollup_lag4, 'lag4 by Vendor-Segment'),\n",
    "    (df_fa2_lag1_top100ATSerror, 'Top 100 DC-Item ATS Error'),\n",
    "    (df_fa2_lag1_top10, 'Top 10 DC-Item Order deltas'),\n",
    "    (df_fa2_lag1_discrep, 'Large Order v Ship Discrep'),\n",
    "    (ATSErrorlogic, 'ATS Error Logic')\n",
    "]\n",
    "\n",
    "# Write all dataframes to their respective sheets in a loop\n",
    "for df, sheet_name in dataframes:\n",
    "    df.to_excel(Filewriter, sheet_name=sheet_name, index=False)\n",
    "\n",
    "# Get the xlsxwriter workbook and worksheet objects\n",
    "workbook = Filewriter.book\n",
    "\n",
    "# Define formats\n",
    "header_format = workbook.add_format({\n",
    "    'bold': True,\n",
    "    'text_wrap': True,\n",
    "    'valign': 'center',\n",
    "    'align': 'center'\n",
    "})\n",
    "currency_format = workbook.add_format({'num_format': '$#,##0.00'})\n",
    "\n",
    "# Function to format each worksheet\n",
    "def format_worksheet(writer, sheet_name, df):\n",
    "    worksheet = writer.sheets[sheet_name]\n",
    "    for col_num, value in enumerate(df.columns):\n",
    "        worksheet.write(0, col_num, value, header_format)\n",
    "        worksheet.set_column(col_num, col_num, len(value) + 2)  # Set column width based on header length\n",
    "        # Apply currency format to columns where header starts with '$'\n",
    "        if value.startswith('$'):\n",
    "            worksheet.set_column(col_num, col_num, None, currency_format)\n",
    "    \n",
    "    worksheet.autofilter(0, 0, 0, len(df.columns) - 1)  # Add dropdown filters\n",
    "    worksheet.freeze_panes(1, 0)  # Freeze the top row\n",
    "\n",
    "# Apply formatting to all worksheets in a loop\n",
    "for df, sheet_name in dataframes:\n",
    "    format_worksheet(Filewriter, sheet_name, df)\n",
    "\n",
    "# Close the writer\n",
    "Filewriter.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
